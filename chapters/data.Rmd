---
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(USAboundaries)
library(patchwork)
`%ni%` <- Negate(`%in%`)

interior_west <- c("AZ", "CO", "ID", "MT", "NV", "NM", "UT", "WY")

states <- data.frame(state.abb) %>%
  filter(state.abb %ni% interior_west & state.abb %ni% c("AK", "HI")) %>%
  pull()

dat_small <- read_csv("../data/subsets/dat_small.csv")
```

# Data

The data used in this thesis was collected by the Forest Inventory and Analysis Program (FIA) in the span of 10 years from 2007 to 2017. While this data was collected over this 10 year period, the analyses done throughout this thesis are under the assumption that this is a "snapshot" of the Interior West at some moment in time. The data we have is plot-level data for the Interior West region of the United States, where the data for each plot is collected by FIA and its crew members. The units measured by the FIA and their ground crews are approximately 30 m by 30 m hexagonal units. Since the Interior West covers over 140 million acres of forestland, it is extremely impractical for FIA to measure every unit (SOURCE: https://www.fs.fed.us/rm/ogden/lib/interiorwest2.pdf). Instead, they sample from the population of 30 m by 30 m hexagonal units by using a geographically-based systematic sampling design. The FIA chooses these samples by first overlaying a hexagonal grid over the United States where each hexagon contains 6000 acres of land. Then, they fill these hexagons with much smaller hexagons and randomly sample from the population of small hexagons. Then, ground crews go to these sampled small hexagons and collect variables such as basal area, trees per acre, etc. 

Again, the data we have is from the Interior West, and the FIA defines the Interior West as Arizona, Colorado, Idaho, Montana, Nevada, New Mexico, Utah, and Wyoming. For reference we have provided the Interior West colored green on a map of the continental United States:

(FIGURE OF INTERIOR WEST)

```{r, echo = FALSE, message = FALSE, warning = FALSE}
ggplot() +
  geom_sf(data = us_boundaries(type = "state",
                               states = interior_west),
          fill = "#597058",
          color = "black") +
  geom_sf(data = us_boundaries(type = "state",
                               states = states),
          fill = "grey",
          color = "black") +
  theme_void()
```


While the data covers the Interior West as a whole, we have very granular information, as each row represents a plot sampled by FIA. The data also includes variables that subset the Interior West into provinces which contain ecosections, and these ecosections contain ecosubsections. In our data, on average, each ecosection contains approximately 7.06 ecosubsections, and each province contains an average of 4.86 ecosections. So, an average province then contains just over 34 ecosubsections. This hierarchical struture of the data allows for us to more easily create hierarchical models which borrow strength from surrounding areas. 

While this data contains a multitude of variables, the analyses done in this thesis focus on four key response variables and two explanatory variables. The response variables used are basal area (square-foot), trees per acre, above-ground biomass (lbs), and net volume (ft^3). The explanatory variables used are remotely sense variables: forest biomass and forest probability. We can look at the log average of these variables across the Interior West region (really want this by ecosubsection... having trouble with the shape file):

```{r, echo = FALSE, message = FALSE, warning = FALSE}
p1 <- ggplot(dat_small) +
  stat_summary_hex(
    fun = function(x) {
      log(mean(x))
    },
    mapping = aes(x = LON_PUBLIC,
                  y = LAT_PUBLIC,
                  z = BIOLIVE_TPA)
  ) +
  geom_sf(
    data = us_boundaries(type = "state",
                         states = interior_west),
    color = "black",
    alpha = 0
  ) +
  theme_void() +
  scale_fill_gradient(low = "#A9875C", high = "#597058") +
  labs(x = "", y = "", fill = "Biomass") +
  theme(
    legend.position = "left"
  )

p2 <- ggplot(dat_small) +
  stat_summary_hex(
    fun = function(x) {
      log(mean(x))
    },
    mapping = aes(x = LON_PUBLIC,
                  y = LAT_PUBLIC,
                  z = BALIVE_TPA)
  ) +
  geom_sf(
    data = us_boundaries(type = "state",
                         states = interior_west),
    color = "black",
    alpha = 0
  ) +
  theme_void() +
  scale_fill_gradient(low = "#A9875C", high = "#597058") +
  labs(x = "", y = "", fill = "Basal Area") 

p3 <- ggplot(dat_small) +
  stat_summary_hex(
    fun = function(x) {
      log(mean(x))
    },
    mapping = aes(x = LON_PUBLIC,
                  y = LAT_PUBLIC,
                  z = CNTLIVE_TPA)
  ) +
  geom_sf(
    data = us_boundaries(type = "state",
                         states = interior_west),
    color = "black",
    alpha = 0
  ) +
  theme_void() +
  scale_fill_gradient(low = "#A9875C", high = "#597058") +
  labs(x = "", y = "", fill = "Count") +
  theme(
    legend.position = "left"
  )

p4 <- ggplot(dat_small) +
  stat_summary_hex(
    fun = function(x) {
      log(mean(x))
    },
    mapping = aes(x = LON_PUBLIC,
                  y = LAT_PUBLIC,
                  z = VOLNLIVE_TPA)
  ) +
  geom_sf(
    data = us_boundaries(type = "state",
                         states = interior_west),
    color = "black",
    alpha = 0
  ) +
  theme_void() +
  scale_fill_gradient(low = "#A9875C",
                       # mid = "#597058", 
                       high = "#597058") +
  labs(x = "", y = "", fill = "Net Volume") 

(p1 + p2) / (p3 + p4)
```











```{r}
#library(sf)
#library(concaveman)

#dat_small <- read_csv("../data/subsets/dat_small.csv")

#y_summarize <- dat_small %>%
#  group_by(subsection) %>%
#  summarize(
#    mean_BIOLIVE_TPA = mean(BIOLIVE_TPA),
#    mean_BALIVE_TPA = mean(BALIVE_TPA),
#    mean_CNTLIVE_TPA = mean(CNTLIVE_TPA),
#    mean_VOLNLIVE_TPA = mean(VOLNLIVE_TPA)
#  )
# 
# ggplot(data = ecomap_section) +
#   geom_sf(position = "geometry")


# dat_small_spatial <- dat_small %>%
#  filter(subsection %ni% c("322Ad", "331Ff", "341Df", "341Di", "322Ab")) %>%
#  st_as_sf(., coords = c("LON_PUBLIC", "LAT_PUBLIC"))

#polys <- st_sf(aggregate(dat_small_spatial$geometry,
 #                        list(dat_small_spatial$subsection),
  #                       function(g) {
   #                        st_cast(st_combine(g), "POLYGON")
    #                     }))

#ggplot(polys) +
 # geom_sf()
```




