---
title: "Literature Review"
author: "Grayson White"
output:
  pdf_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Literature Review

### Ver Plank et al (2017): Hierarchical Bayesian models for small area estimation of forest variables using LiDAR

This paper proposes three Hierarchical Bayesian (HB) models for small area estimation (SAE) of above ground biomass (AGB). These models are the Fay-Herriot (FH), Fay-Herriot with conditional autoregressive random effects (FHCAR), and FHCAR with smoothed sampling variance. They first introduce the FH model with is defined as: 
$$
Y_i = \theta_i + \epsilon_i 
$$
$$
\theta_i = x_i \beta + v_i
$$

where $\theta_i$ is the mean AGB, $\epsilon_i \sim N(0,\sigma^2_i)$, the $x_i$ is a $(p \times 1)$ matrix, $\beta$ is the $(p\times 1)$ matrix of regression coefficients, and $v_i \sim N(0, \sigma^2_v)$. Note: $1\leq i \leq m$ where we are indexing over forest stands. 

The FHCAR model is very similar, except for the fact that it adds a spatially structured random effect
that follows a conditional autoregressive (CAR) prior distribution which allows the authors to take advantage of the spatial autocorrelation seen in the data (stands close together have similar AGB values). For this model, they specify:
$$
v = (v_1, \dots, v_m) \sim N(0, \Sigma (\sigma^2_v, \lambda))
$$

where $\lambda$ is the autocorrelation parameter. They also specify the definition of the covariance matrix:
$$
\Sigma (\sigma^2_v, \lambda) = \sigma^2_v[\lambda R + (1-\lambda)I]^{-1}
$$

The third model they specify is again similar to the first two, but with more: they want to be able to reduce instability in variance estimates in small sample sizes and so instead of saying that we have a fixed and known sampling variance (which is common practice), they specify the FHCAR-SMOOTH model where they define the following: 
$$
\tilde{\sigma_i^2} = \frac{V_e}{n_i}
$$
$$
V_e = \frac{\sum_{i=1}^{m} a_i \sigma^2_i}{\sum_{i=1}^{m} a_i}
$$
where $n_i$ is the number of variable radius plots in stand $i$. 

After the authors introduce these three models, they discuss the priors used in their analysis. They use flat priors for all $\beta$'s, $\sigma^2_v \sim InvGamma(\text{shape} = 2, \text{scale} = \sum_{i=1}^{m} \sigma^2_i /m)$, and $\lambda \sim Unif(0,1)$ To sample from the posterior for $\theta$, $\beta$, and $\sigma^2_v$ the authors used the Gibbs sampler, and for $\lambda$ they used the Metropolis-Hastings algorithm.

After running their models, they compared their results to Breidenbach et al. (2016) and Mauro et al. (2016) and those author's frequentist results. The HB models had higher $\sigma^2_v$ which allows for more realistic inference dealing with the uncertainty associated with estimating above ground biomass. 


### Breidenbach (2012) (Frequentist methods): Small area estimation of forest attributes in the Norwegian National Forest Inventory

This study's goal is the measure mean forest biomass in a forest of Vestfold County, Norway. The authors compared simple random sampling, generalized regression, and EBLUP estimators. For the simple random sampling estimator, the authors calculate the sample mean, however they note that the MSE is unstable due to the small number of random samples. The next estimator that they discuss is the synthetic regression estimator which is an indirect estimator. This estimator is "synthetic" because it "synthesizes information from sample plots also outside the domain of interest." They estimate the mean with the following:
$$
\bar{Y}_{S,i} = \sum_{j=1}^{N_i} \vec x_{ij}^T \vec\beta
$$
For this estimator, they do not calculate the MSE because "the domain-level model bias cannot be considered adequately with the existing MSE estimators, which is why we will not derive MSE estimates for the SRE." To deal with the bias, the authors introduced the generalized regression estimator (GREG) which uses a correction term that accounts for bias given $n$ is large enough.

They next discuss the BLUP estimator. This is a model based estimator rather than a direct or indirect estimator and it combines direct and indirect estimates. The BLUP estimator is very similar to the GREG estimator, except for the factor: 
$$
\gamma_i = \frac{\sigma^2_v}{\sigma^2_v + \sigma^2_\epsilon / n_i}
$$
which handles the weight of the bias correction factor according to model accuracy and $n$. Finally, the EBLUP estimator uses estimated variances rather than the true value of the variances as in the BLUP model.

The authors found that the MSE of the EBLUP estimator was much lower than that of the SRS given a reliable estimate was possible. Most of the time, the MSE of the EBLUP was smaller than that of the GREG as well. The authors conclude to favor the EBLUP over the two other SAE methods (GREG and SRS).


### Finley (2009): Improving the performance of predictive process modeling for large datasets

In this paper, Finley proposes a knot-based predictive process aimed to reduce computation time and preserve "the richness of desired hierarchical spatial modeling specifications in the presence of large datasets." Included in the paper is a section on the application to forest biomass prediction and modeling. When applying this method, they found: "Convergence diagnostics revealed 5000 iterations to be sufficient for initial burn-in and so the remaining 30,000 samples from each chain were used for posterior inference. The 206 knot model required approximately 2 h to complete the MCMC sampling with the 106 and 51 knot models requiring substantially less time to collect the specified number of samples" and concluded that "Ultimately, the predictive process model makes this analysis and subsequent pixel-level prediction trivial for even a common single processor workstation."

I am unsure if this computation barrier will become relevant when doing small area estimation, however if it does this knot-based approach seems extremely relevant to look into and potentially implement.


