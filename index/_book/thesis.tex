% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See https://www.reed.edu/cis/help/LaTeX/index.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in
% the 2002-2003 Senior Handbook. Ask a librarian to check the
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: https://www.ctan.org/
%%
\usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs,setspace}
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
% Added by CII
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{float}
\floatplacement{figure}{H}
% Thanks, @Xyv
\usepackage{calc}
% End of CII addition
\usepackage{rotating}

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
% End of CII addition

% Added by CII
\usepackage{caption}
\captionsetup{width=5in}
% End of CII addition

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino

% Syntax highlighting #22
  \usepackage{color}
  \usepackage{fancyvrb}
  \newcommand{\VerbBar}{|}
  \newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
  % Add ',fontsize=\small' for more characters per line
  \usepackage{framed}
  \definecolor{shadecolor}{RGB}{248,248,248}
  \newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
  \newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
  \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
  \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\BuiltInTok}[1]{#1}
  \newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
  \newcommand{\ExtensionTok}[1]{#1}
  \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ImportTok}[1]{#1}
  \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\NormalTok}[1]{#1}
  \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\RegionMarkerTok}[1]{#1}
  \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

% To pass between YAML and LaTeX the dollar signs are added by CII
\title{A Hierarchical Bayesian Approach to Small Area Estimation of Forest Attributes}
\author{Grayson White}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2021}
\division{Mathematics and Natural Sciences}
\advisor{Kelly McConville}
\institution{Reed College}
\degree{Bachelor of Arts}
%If you have two advisors for some reason, you can use the following
% Uncommented out by CII
% End of CII addition

%%% Remember to use the correct department!
\department{Mathematics}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

% From {rticles}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
% for Pandoc 2.8 to 2.10.1
\newenvironment{cslreferences}%
  {}%
  {\par}
% For Pandoc 2.11+
% As noted by @mirh [2] is needed instead of [3] for 2.12
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc} % for calculating minipage widths
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\renewcommand{\contentsname}{Table of Contents}
% End of CII addition

\setlength{\parskip}{0pt}

% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\Acknowledgements{
This is where acknowledgements will go.
}

\Dedication{
To my family.
}

\Preface{
This is an example of a thesis setup to use the reed thesis document class
(for LaTeX) and the R bookdown package, in general.
}

\Abstract{
The preface pretty much says it all.

\par

Second paragraph of abstract starts here.
}

% End of CII addition
%%
%% End Preamble
%%
%
\begin{document}

% Everything below added by CII
  \maketitle

\frontmatter % this stuff will be roman-numbered
\pagestyle{empty} % this removes page numbers from the frontmatter
  \begin{acknowledgements}
    This is where acknowledgements will go.
  \end{acknowledgements}
  \begin{preface}
    This is an example of a thesis setup to use the reed thesis document class
    (for LaTeX) and the R bookdown package, in general.
  \end{preface}
  \hypersetup{linkcolor=black}
  \setcounter{secnumdepth}{2}
  \setcounter{tocdepth}{2}
  \tableofcontents

  \listoftables

  \listoffigures
  \begin{abstract}
    The preface pretty much says it all.

    \par

    Second paragraph of abstract starts here.
  \end{abstract}
  \begin{dedication}
    To my family.
  \end{dedication}
\mainmatter % here the regular arabic numbering starts
\pagestyle{fancyplain} % turns page numbering back on

\hypertarget{context}{%
\chapter{Context}\label{context}}

National forest inventories such as the United States Forest Inventory and Analysis Program (FIA) monitor the status of a nation's forests by collecting data and estimating forest attributes such as basal area, above-ground biomass, tree count per acre, and net volume. Due to the sheer amount of forests in the United States, the FIA cannot collect the population data for these variables across the United States. Instead, they use a sampling design intended and well-suited for estimation over large geographic regions such as states. This sampling design works very well for estimation in large regions and maintains a reasonable cost of employing foresters to collect the samples. While this method works sufficiently for large areas, it has become an interest of national forest inventories such as the FIA to be able to provide reliable and efficient estimates of forest attributes in small domains such as ecological subsections (often referred to as eco-subsections) and counties. In particular, the FIA would like to have estimates with low variance in eco-subsections, however, the FIA only samples a small numbers of plots in these small areas. Currently, the FIA's standard approach to this problem is by using post-stratification. Post-stratification uses a weighted average of the forest attribute of interest and corrects for over- or under-sampling of forested land in the small area. While this estimator is unbiased and has lower variance than the mean, it does not reduce the variance enough for precise estimation at the eco-subsection level. The research goal of this thesis is to address this problem by using techniques which seek to minimize estimate variance while only introducing a small amount of bias. Having precise estimates of forest attributes at the eco-subsection level is crucial for educational programs and implementation of programs which seek to maintain the health of our forests.

In order to produce these estimates we must perform small area estimation. Small area estimation is a branch of survey statistics which includes techniques that allow us to estimate the value of parameters at a sub-population level. Typically in survey estimation, we are interested in doing inference at a population level, however we are sometimes interested in attaining estimates for sub-populations or ``small areas.'' We can visualize the process by considering an ecological province divided into three eco-subsections, each of which have sampled areas:
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/ecoprov-diagram} 

}

\caption[An ecological province]{An ecological province containing three eco-subsections. The red, yellow, and seafoam areas represent eco-subsections. The green hexagons represent forested areas sampled by the FIA. The beige hexagons represent non-forested areas sampled by the FIA.}\label{fig:ecological-province-diagram}
\end{figure}
We are interested in performing inference at the sub-population level, and in Figure \ref{fig:ecological-province-diagram} these sub-populations are represented by the red, yellow, and seafoam green areas. Importantly, we want to attain estimates of forest attributes in each of these sub-populations. There are wide range of techniques that can be used to carry out this small area estimation. Broadly, these methods fall into three categories: direct estimators, indirect estimators with implicit models, and indirect estimators with explicit models. We will often refer to indirect models with explicit models as ``model-based estimators.'' Each of these methods attempts to do inference at the sub-population level, however, they are quite different from each other.

Direct estimators are defined as those that rely only on the samples within the small area which we would like to measure. Some examples of a direct estimator are the sample mean or the post-stratified estimator. The post-stratified estimator is similar to the mean, however it accounts for under- and over-sampling of forested areas in a given sub-population. These estimators do not rely on information outside of the small area being estimated, however, the post-stratified estimator uses auxiliary information such as the true proportion of forested land within the small area to produce estimates. Direct estimation is the simplest kind of small area estimator as it only relies on samples within the sub-population of interest to produce its estimates. We can visualize these two estimators to get a better sense of their estimation process by considering how they would estimate some forest attribute \(y\) in our seafoam green sub-population \(j\) from Figure \ref{fig:ecological-province-diagram}.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/mean-diagram} 

}

\caption[The mean as a direct estimator]{The mean as a direct estimator in the seafoam green eco-subsection. This estimator only relies on sampled areas (hexagons) within the eco-subsection. This estimator does not take into account whether the sampled areas are forested (green) or non-forested (beige) areas. This estimator does not use a model to produce estimates and hence uses the \textit{y} variable collected by FIA foresters to produce the needed estimate, as shown inside the hexagons.}\label{fig:mean-diagram}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/ps-diagram} 

}

\caption[The post-stratified direct estimator]{The post-stratified direct estimator in the seafoam green eco-subsection. This estimator only relies on sampled areas (hexagons) within the eco-subsection. This estimator takes into account whether the sampled areas are forested (green) or non-forested (beige) areas. It then weights our estimate based on the true population's proportion of forested area in the eco-subsection. This estimator does not use a model to produce estimates and hence uses the \textit{y} variable collected by FIA foresters to produce the needed estimate, as shown inside the hexagons.}\label{fig:ps-diagram}
\end{figure}
The second kind of estimator, indirect estimators with implicit models, rely on data outside of the area of interest to produce their estimate and can rely on auxiliary data, but implement a model implicitly. With implicit, model-based, indirect estimators we can use information (or ``borrow strength'') from nearby small areas to help improve our estimate (i.e.~reduce variance) in our area of interest through implicit use of a model. These indirect estimators are quite a bit more complicated than direct estimators due to the fact that they borrow strength from the variable of interest across small areas, however, they often significantly reduce variance in estimates due to the added information from other sub-populations. According to Rao (2014), while indirect estimators with implicit models reduce variance, they are often design biased due to the inability to specify between-area variation. This is a large cost of implementing implicit, model-based, indirect estimators. Further, these estimators do not reduce variance as significantly as explicit model-based estimators. Thus, we will not implement indirect estimators with implicit models in this thesis.

Finally, explicit model-based estimators are those which both borrow strength from other small areas, use auxiliary information, and explicitly use a model to compute the estimate of interest. These estimators are still within the family of indirect estimators. However they make explicit use of a model. Similarly to the indirect estimators with implicit models discussed previously, these models can further reduce the variance of our estimates because they allow for more information to be used in the estimate. We can further categorize these explicit model-based estimators into two classes: unit-level and area-level models. Unit-level models consider information at the level of which the data was collected. Area-level models consider information that has been aggregated to the level of a small area before the model is fit to the data. Commonly, the empirical best linear unbiased prediction model (EBLUP) is used in small area estimation as the model-based estimator of choice. This is because both the area- and unit-level EBLUP models reduce variance further than direct and indirect estimators with implicit models, and they are design unbiased given the modeling assumptions are met. This thesis primarily investigates the usefulness of the hierarchical Bayesian unit-level model (HBU) and hierarchical Bayesian area-level model (HBA). We can visualize HBU and HBA estimators to give a better sense of the differences between the two.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/hbu-diagram} 

}

\caption[The unit-level hierarchical Bayesian model]{The unit-level hierarchical Bayesian model producing an estimate in our seafoam green eco-subsection. This estimator relies on auxiliary information from all eco-subsections within a given eco-province to produce estimates as shown by the arrows. Notably, more information is used in the eco-subsection of interest to produce the estimate, denoted by the solid arrows. The dashed arrows tell us that less information is being used from outside eco-subsections. This estimator also produces estimates based on remotely sensed data, as denoted by the \textit{x} variables in each hexagon. This estimator uses information at the unit-level, meaning that it produces estimates from the plot level of granularity.}\label{fig:hbu-diagram}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/hba-diagram} 

}

\caption[The area-level hierarchical Bayesian model]{The area-level hierarchical Bayesian model producing an estimate in our seafoam green eco-subsection. This estimator relies on auxiliary information from all eco-subsections within a given eco-province to produce estimates as shown by the arrows. Notably, more information is used in the eco-subsection of interest to produce the estimate, denoted by the solid arrows. The dashed arrows tell us that less information is being used from outside eco-subsections. This estimator also produces estimates based on remotely sensed data, as denoted by the \textit{x} variables in each hexagon. This estimator uses information at the area-level, meaning that it produces estimates based on the post-stratified estimate in each eco-subsection.}\label{fig:hba-diagram}
\end{figure}
It is important to note that while Figures \ref{fig:hbu-diagram} and \ref{fig:hba-diagram} describe the hierarchical Bayesian estimators, the diagrams would be the same for the unit- and area-level EBLUP estimators. This is due to the fact that the EBLUP estimators assume a frequentist mixed model rather than a Bayesian one. These estimators are of course different, we just do not explicitly show how the magnitude of strength borrowed is decided in Figures \ref{fig:hbu-diagram} and \ref{fig:hba-diagram}.

We can see that both the hierarchical Bayesian unit- and area-level models borrow strength from surrounding areas and explicitly model the \(y\) variable outcome as a function of remotely sensed \(x\) variable(s). The notable difference between the two models is that the hierarchical Bayesian unit-level models borrows strength from the unit-level data while the area-level model borrows strength from data aggregated by the post-stratified direct estimator.

Explicit model-based estimation has been increasing in popularity in the realm of applications to the FIA and forestry data in general. As the FIA requires a reduction in variance for their estimates of increasingly smaller areas, it becomes inevitable that borrowing strength from surrounding areas, the use of auxiliary data, and the explicit use of a model is needed to maintain a satisfactory amount of variance. Commonly, frequentist model-based estimators are used for model-based small area estimation, such as the EBLUP estimator. Models such as the EBLUP have some very nice properties. Most notably, they are ``unbiased'' if the assumed model is correct. To understand what it means to have an ``unbiased'' estimator, we must first define bias of some estimator \(\hat\mu_{y_j}\) of a parameter \(\mu_{y_j}\):
\begin{align}
\text{Bias}(\hat\mu_{y_j}) = E[\hat\mu_{y_j}] - \mu_{y_j}
\end{align}
It intuitively follows that if the modeling assumptions are met and our estimator \(\hat\mu_{y_j}\) is unbiased that we will have the following property:
\begin{align}
\text{Bias}(\hat\mu_{y_j}) = E[\hat\mu_{y_j}] - \mu_{y_j} = 0
\end{align}
That is, the expected value of the estimator, \(\hat\mu_{y_j}\), is in fact the true value of the forest attribute of interest. It is clear as to why this is a trait we would want in our model and to why it is so commonly used, however, what is not clear is the cost of this trait. By only focusing on reducing the bias in our estimates, we must ignore the second piece of the mean squared error, the variance. While it is important for bias to be low, we can often reduce our mean squared error by a large amount by increasing bias slightly, as bias and variance are inversely related. We can see by the representation of the mean squared error (MSE) as the sum of the variance and squared bias of our estimator:
\begin{align}
\text{MSE}(\hat\mu_{y_j}) = \text{Var}(\hat\mu_{y_j}) + \text{Bias}(\hat\mu_{y_j},~ \mu_{y_j})^2
\end{align}
This thesis explores this trade-off between bias and variance in depth. We implement hierarchical Bayesian unit- and area-level models which allow for the estimates to be slightly biased while reducing variance. Throughout this thesis, we compare these techniques to small area estimations methods such as the EBLUP and the post-stratified direct estimator. By applying these models on four response variables across the entire Interior West at the eco-subsection level, we can add a great deal of understanding to the usefulness of hierarchical Bayesian models in a small area estimation context, especially when considering its usefulness to the FIA and other forestry organizations. We only have been able to source one paper which uses hierarchical Bayesian modeling for small area estimation with a forestry application, and they only consider the area-level model with a particular response variable in particular forest (Ver Planck, Finley, \& Huff, 2017). This thesis thus adds significantly to our understanding of the usefulness of hierarchical Bayesian small area estimation in a forestry setting due to the introduction of the unit-level model, the vast number of response variables studied, and the vast range of area where we test the usefulness of this model.

\hypertarget{data}{%
\chapter{Data}\label{data}}

\hypertarget{the-forest-inventory-analysis-program}{%
\section{The Forest Inventory \& Analysis Program}\label{the-forest-inventory-analysis-program}}

The FIA is a program within the United States Forest Service which aims to collect information and data in order to assess the country's forests. The FIA has been continuously operating since 1930 and their official mission is to ``make and keep current a comprehensive inventory and analysis of the present and prospective conditions of and requirements for the renewable resources of the forest and rangelands of the US'' (FIA, 2020).

The FIA collects data all throughout the United States by completing a survey each year of many plots of land. The units measured by the FIA and their ground crews are approximately 30 meter by 30 meter hexagonal units. Due to the vast size of the United States and immense amount of forested land, it would be nearly impossible for the FIA to attain population data for the country, so they use sampling instead. The FIA samples from the population of 30 meter by 30 meter hexagonal units by using a geographically-based systematic sampling design (K. S. McConville, Moisen, \& Frescino, 2020). The FIA chooses these samples by first overlaying a hexagonal grid over the United States where each hexagon contains approximately 6000 acres of land. Then, they fill these hexagons with much smaller hexagons and randomly sample from the population of small hexagons. Then, ground crews go to these sampled small hexagons and collect variables such as basal area, trees per acre, etc. Along with this hand-collected data from FIA ground crews, the FIA also uses remotely sensed data to gain more information about the areas which they collect data. For example, the \texttt{nlcd11} variable, which measures total percent tree canopy cover of a plot, is collected via remote sensing by the Multi-Resolution Land Characteristics Consortium (Homer, 2015). Throughout the duration of the thesis, we will be working to predict ground-collected data with remotely sensed variables, such as \texttt{nlcd11}. Having remotely sensed variables like \texttt{nlcd11} is useful to us and the FIA because if our models can predict ground-collected variables well, the FIA can collect less data and have a larger effective sample size.

\hypertarget{the-interior-west}{%
\section{The Interior West}\label{the-interior-west}}

While the FIA collects data in all regions of the United States, the analyses done in this thesis uses data from the Interior West Forest Inventory and Analysis Unit (IW-FIA). Data from this unit will henceforth be referred to as data from ``the Interior West.'' The Interior West is defined as a broad region of the United States, covering the states of Arizona, Colorado, Idaho, Montana, Nevada, New Mexico, Utah, and Wyoming. For reference we have provided the Interior West colored green on a map of the continental United States:
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/interior-west-on-usa} 

}

\caption{The Interior West region of the United States}\label{fig:unnamed-chunk-3}
\end{figure}
The IW-FIA collects annual inventories of the Interior West, with the goal of covering 10\% of the region each year, so every decade the IW-FIA should have measurement of 100\% of each Interior West state's forests.

The Interior West region itself contains the states which encompass the Rocky Mountains along with some other smaller mountain ranges. The Interior West contains 855,767 square miles of land which has an extremely diverse landscape ranging from the high mountain peaks of the Rockies to flat desert plains in Nevada and other Interior West states. Along with desert and mountains, the Interior West also includes parts of the Great Plains. Throughout this diverse landscape, there is a similarly diverse range of forested areas. The forested areas range from areas that are humid and temperate to areas like the Northern Rocky Mountain Forest which is dry and considered a temperate desert.

\hypertarget{our-data-specifics}{%
\section{Our Data: Specifics}\label{our-data-specifics}}

The data used in this thesis was collected by the FIA in the span of 10 years from 2007 to 2017. While this data was collected over this 10 year period, the analyses done throughout this thesis are under the assumption that this is a ``snapshot'' of the Interior West at some moment in time. Thus we do not consider any temporal features of this dataset, however the inventory year information is available to us. The data we have is plot-level (sometimes referred to as ``unit-level'') data for the Interior West region of the United States, where the data for each plot consists of ground data collected by FIA and remotely sensed data.

The dataframe used in this thesis is a joined dataframe derived from two FIA datasets of the Interior West, \texttt{spatial} and \texttt{response}. The \texttt{spatial} dataframe contains 89444 observations and 70 variables, most notably our remotely sensed predictor variable (\texttt{nlcd11}), location information, and eco-subsection. The \texttt{nlcd11} variable was collected by the Multi-Resolution Land Characteristics Consortium (Homer, 2015). This variable measures percent tree canopy cover in a given plot.

The \texttt{response} dataframe contains 86085 observations and 67 variables, most notably four response variables collected by FIA crew members (\texttt{BALIVE\_TPA}, \texttt{CNTLIVE\_TPA}, \texttt{BIOLIVE\_TPA}, and \texttt{VOLNLIVE\_TPA}), location information, and eco-subsection. The response variables noted above measure basal area, tree count, biomass, and volume, respectively. We join these dataframes by their unique plot number, and subset the number of variables significantly to 19 variables which contain plot information, longitude \& latitude, elevation, predictor variables, response variables, eco-subsection, eco-section, and eco-province. The resulting joined dataframe has 86085 rows as these are the rows which share the same plots between the \texttt{response} and \texttt{spatial} dataframes. We can see the first few rows of the dataframe with relevant columns selected and values rounded to the second decimal place:
\begin{longtable}[t]{rrrrrl}
\caption[Relevant Glimpse of Data]{\label{tab:unnamed-chunk-4}Relevant Glimpse of Data}\\
\toprule
Plot & Latitude & Longitude & nlcd11 & BIOLIVE\_TPA & subsection\\
\midrule
83574 & -109.71 & 32.85 & 21 & 0.00 & 321Af\\
84904 & -109.88 & 32.99 & 0 & 0.00 & 321Af\\
83021 & -109.88 & 32.81 & 0 & 0.00 & 321Aj\\
82635 & -109.89 & 32.65 & 26 & 14.74 & 321Am\\
90381 & -109.83 & 32.62 & 41 & 31.50 & 321Am\\
\addlinespace
81801 & -109.79 & 32.35 & 0 & 0.00 & 321Aj\\
\bottomrule
\end{longtable}
While the data covers the Interior West as a whole, we have very granular information, as each row represents a plot sampled by the FIA. The data also includes variables that subset the Interior West into provinces which contain eco-sections, and these eco-sections contain eco-subsections. In our data, on average, each eco-section contains approximately 7.06 eco-subsections, and each province contains an average of 4.86 eco-sections. So, an average province then contains just over 34 eco-subsections. We can take a look at the Northern Rocky Forest province, colored by eco-section, with lines dividing each eco-subsection to see this structure in action:
\begin{figure}

{\centering \includegraphics{thesis_files/figure-latex/northern-rocky-1} 

}

\caption{The Northern Rocky Forest colored by eco-section}\label{fig:northern-rocky}
\end{figure}
The data we have covers a total of 14 provinces, 68 eco-sections, and 480 eco-subsections. The hierarchical structure of the data and nested nature of the eco-subsections within eco-sections within eco-provinces lends itself to be able to create hierarchical models which borrow strength from surrounding areas.

While this data contains a multitude of variables, the analyses done in this thesis focus on four key response variables and one explanatory variable. The response variables used are basal area (square-foot), trees per acre, above-ground biomass (lbs), and net volume (\(\text{ft}^3\)). These variables are coded as \texttt{BALIVE\_TPA}, \texttt{CNTLIVE\_TPA}, \texttt{BIOLIVE\_TPA}, and \texttt{VOLNLIVE\_TPA}, respectively. We can look at the average of these variables across the Interior West region by eco-subsection in the four following maps of the interior west.
\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{figure/basal} 

}

\caption{Mean basal area in Interior West eco-subsections}\label{fig:unnamed-chunk-5}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{figure/biomass} 

}

\caption{Mean biomass in Interior West eco-subsections}\label{fig:unnamed-chunk-6}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{figure/count} 

}

\caption{Mean tree count per acre in Interior West eco-subsections}\label{fig:unnamed-chunk-7}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.65\linewidth]{figure/voln} 

}

\caption{Mean net volume in Interior West eco-subsections}\label{fig:unnamed-chunk-8}
\end{figure}
While we have four variables which we will model as response variables throughout the analyses, we also have one predictor variables which will be of much use to us. In particular, total tree canopy cover (coded as \texttt{nlcd11}.) This variable is remotely sensed, meaning that they were not collected by FIA crew members, but rather with aerial photography and/or satellite imagery. However, we will be using these variables to attempt to predict our response variables in order to understand how good of estimates we can make with this remote data that does not require as much effort to collect. Notably, we can consider this variable to contain the population totals for total canopy cover as the data is collected for the entire Interior West region.

To get a sense of a our predictor variable \texttt{nlcd11}, we will look at its distribution in the Northern Rocky Forest subset of our data compared to its distribution across the entire Interior West:
\begin{figure}

{\centering \includegraphics[width=432px]{thesis_files/figure-latex/hists-1} 

}

\caption[Total canopy cover in the M333 eco-province and Interior West]{Distribution of total canopy cover in the M333 eco-province (left) and the entire Interior West (right)}\label{fig:hists}
\end{figure}
Notably, the Northern Rocky Forest Province (M333) is much more forested than the Interior West, so we see much different distributions of total canopy cover in this subset of the data. Apart from making these histograms, we can also summarize the entire, unit-level data and see some summary statistics of our five key variables:
\begin{longtable}[t]{lrrrrrr}
\caption[Summary Statistics of Relevant Variables]{\label{tab:var-tab}Summary Statistics of Relevant Variables}\\
\toprule
Variable & Mean & SD & Median & 75th Percentile & Min & Max\\
\midrule
nlcd11 & 8.73 & 18.57 & 0 & 0.00 & 0 & 95.00\\
BIOLIVE\_TPA & 6.23 & 16.84 & 0 & 1.98 & 0 & 244.35\\
BALIVE\_TPA & 22.75 & 48.06 & 0 & 14.75 & 0 & 469.39\\
CNTLIVE\_TPA & 98.60 & 283.09 & 0 & 30.09 & 0 & 6677.93\\
VOLNLIVE\_TPA & 342.32 & 972.78 & 0 & 74.69 & 0 & 16435.55\\
\bottomrule
\end{longtable}
From this table, we can see how heavily skewed these key variables are, with all the variables having median of zero. This does not stop us from doing meaningful analyses though, as the sample size of this dataset is so large (\(n = 86085\)) and thus we have plenty of data to create models with.

Finally, we also have population data showing the proportion of each eco-subsection that is forested. This data allows us to create our post-stratified estimates which are discussed in detail in the following chapter.

\hypertarget{data-structure-hierarchy}{%
\section{Data Structure \& Hierarchy}\label{data-structure-hierarchy}}

As hinted at throughout earlier parts of the chapter, the data used in this thesis has a hierarchical structure, where eco-subsections are nested within eco-sections which are in turn nested within eco-provinces. Every plot has each level of granularity of location data recorded and this is what allows us to choose how far to borrow strength from other plots. We can see this structure of nested data by looking at a diagram depicting this data structure:
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figure/data-structure} 

}

\caption[The nested data structure of the Interior West]{The nested data structure of the Interior West. The green area is the eco-province which is divided into eco-sections (red area) which is in turn divided into eco-subsections (beige area).}\label{fig:unnamed-chunk-9}
\end{figure}
The largest motivation for hierarchical modeling in this particular application is that observations are more similar within the hierarchies which we split them into. To understand if this is true, we can do a preliminary analysis on the data by performing three-way ANOVAs for each key variable with predictors \texttt{province}, \texttt{section}, and \texttt{subsection}. For succinctness, we can look at the ANOVA results for one of the response variables, \texttt{BIOLIVE\_TPA}, but the other variables tell a very similar story in terms of homogeneity. By just looking at the MSE of the ANOVA results, we can see that we should expect more homogeneity within eco-subsections:
\begin{longtable}[t]{lrrrrr}
\caption{\label{tab:unnamed-chunk-10}Analysis of Variance Model (Biomass Response)}\\
\toprule
term & df & sumsq & meansq & statistic & p.value\\
\midrule
province & 13 & 6512457.0 & 500958.2335 & 2921.45793 & 0\\
section & 54 & 967169.1 & 17910.5394 & 104.44960 & 0\\
subsection & 412 & 2247964.7 & 5456.2249 & 31.81928 & 0\\
Residuals & 85605 & 14679153.5 & 171.4754 & NA & NA\\
\bottomrule
\end{longtable}
These results allow us to conclude that it is reasonable to believe that observations within a given eco-province are more homogeneous than observations throughout the Interior West. Thus, if we want eco-subsection level estimates of variables, it makes sense to borrow information from other eco-subsections within the same province as each other. This data structure and homogeneity within provinces is what drives the analyses done henceforth in this thesis.

\hypertarget{methods}{%
\chapter{Methods}\label{methods}}

Currently, there are three main types of estimators used to estimate the value of forest attributes: direct, indirect with implicit models, and indirect with explicit models. Direct estimators are commonly thought of as the simplest estimators as they do not borrow strength across small areas for estimation. Direct estimators are hence easy to use and interpret, but we often do not get precise enough estimates with these estimators, in other words, they have high variance. Indirect estimators with implicit models borrow strength implicitly across small areas to produce estimates. These estimators decrease variance by providing a link to related small areas through supplementary data (Rao, 2014). Finally, indirect estimators that make explicit use of a model, or ``model-based estimators,'' aim to reduce variance in estimates by using auxiliary data and making specific allowance for between area variation. As explained in Rao (2014), these model-based estimators have significant advantages over direct estimators and implicit indirect estimators. Notably, model diagnostics can be used, small area-specific measures of precision can be attained (in our case, we compare the coefficient of variation between estimators), and we can used mixed or hierarchical models to capture the dependence structure in the data.

This thesis explores the application of two model-based estimators, the hierarchical Bayesian unit-level model and the hierarchical Bayesian area-level model. These hierarchical Bayesian models are not commonly used in forest inventory research, however these models are often applied in a variety of other application areas ranging from ophthalmology to economic growth to tracking the spread of invasive species, to name a few (Hooten \& Wikle, 2008; Iriawan \& Yasmirullah, 2019; Tojtovska, Ribarski, \& Ljubic, 2019). We compare these novel Bayesian models to the frequentist EBLUP unit- and area-level models and two common direct estimators, the sample mean and the post-stratified estimator. To compare these estimators, we will apply them over many ecological provinces in the Interior West and study their performance when considering four key response variables with one explanatory variable.

In order to explore these estimators in depth, we must introduce notation relevant to them. First of all, our indices will work as follows: \(i\) indexes over units sampled; \(j\) indexes over eco-subsections or ``small areas''; and \(k\) indexes over strata. Now, we are interested in estimating the mean of some response variable \(y\), such as trees per acre or biomass, in a small area. So, let \(\mu_{y_j}\) be the population mean of the study variable in eco-subsection \(j\) in the Interior West. To denote the estimate produced of \(\mu_{y_j}\) we will use \(\hat\mu_{y_j}\) with a superscript denoting which estimator is being used. We will also use \(V(\hat\mu_{y_j})\) to denote the variance of \(\hat\mu_{y_j}\). In summary, each of our estimators aims to estimate \(\mu_{y_j}\), the population mean of some study variable in the \(j\)th eco-subsection and our estimated value is denoted by \(\hat\mu_{y_j}\) and its variance is denoted by \(V(\hat\mu_{y_j})\). We also must introduce \(s_j\), which is a set. The set \(s_j\) includes all units sampled within eco-subsection \(j\). We also introduce \(U_j\), the set of all the area in subsection \(j\). The ``\(U\)'' is chosen as it stands for ``universe.'' Also, we introduce \(n_j\), this denotes the number of sampled units within an eco-subsection \(j\), i.e.~the cardinality of \(s_j\).

\hypertarget{direct-estimation}{%
\section{Direct Estimation}\label{direct-estimation}}

There are two direct estimators that we implement throughout this thesis: the sample mean (a.k.a. the Horvitz-Thompson estimator) and the post-stratified estimator. While the sample mean is an intuitive choice for estimating the population mean of a variable of interest \(y\), the post-stratified estimator helps correct for over- and under-sampling of forested areas. We also introduce a direct estimator which helps create intuition for the unit-level frequentist model, however we do not fit this model to our data. We will now explore the mathematics behind these estimators in depth.

\hypertarget{the-horvitz-thompson-estimator}{%
\subsection{The Horvitz-Thompson Estimator}\label{the-horvitz-thompson-estimator}}

What might be the most intuitive approach to estimating the population mean \(\mu_{y_j}\) is taking the sample mean, i.e.~using the ``Horvitz-Thompson estimator'' as survey samplers like to say (Horvitz \& Thompson, 1952). This estimator can be expressed as follows:
\begin{align}
\hat\mu_{y_j}^{HT} = \frac{1}{n_j} \sum_{i \in s_j} y_i
\end{align}
Ignoring the finite population correction, the variance estimate for Horvitz-Thompson estimator is calculated by:
\begin{align}
\hat V\Big(\hat\mu_{y_j}^{HT}\Big) = \frac{1}{n_j-1} \sum_{i \in s_j} \Big(y_{ij} - \hat\mu_{y_j}^{HT}\Big)^2
\end{align}
Recall that the Horvitz-Thompson estimator is just taking the mean of the study variable of interest, \(y\). Note that \(y_i\) represents the value of the study variable in the \(i\)th unit sampled of eco-subsection \(j\). This estimator is useful as it is easy to compute and does not require any auxiliary information. However, the Horvitz-Thompson estimator high variance relative to other estimators we will discuss. The post-stratified estimator begins to address the variance of the Horvitz-Thompson estimator.

\hypertarget{the-post-stratified-estimator}{%
\subsection{The Post-Stratified Estimator}\label{the-post-stratified-estimator}}

The post-stratified estimator is very similar to the Horvitz-Thompson estimator however, as stated above, it addresses variance that occurred from using the Horvitz-Thompson estimator. While decreasing variance seems like a no cost solution to some of our problems, the post-stratified estimator requires auxiliary information in order to be used. The post-stratified estimator is a weighted sum of two Horvitz-Thompson estimators: one Horvitz-Thompson estimator giving the estimate of the mean in sampled units which are forested and the other Horvitz-Thompson estimator in non-forested areas. We then weight these estimates by the true proportion of area in the eco-subsection of interest that is forested. Note that while we are using auxiliary information such as the true proportion of forested area in the eco-subsection of interest and whether or not the sampled units were forested areas or not, both of these pieces of information only consider the eco-subsection of interest. Therefore, since information is not used from outside of the eco-subsection of interest, the post-stratified estimator is still within the family of direct estimators. We can represent the post-stratified estimator as follows:
\begin{align}
\hat\mu_{y_j}^{PS} = \sum_{k=1}^{2} w_{jk} \cdot \hat\mu_{y_{jk}}^{HT}
\end{align}
Ignoring the finite population correction, the variance estimate for post-stratified estimator is calculated by:
\[
\hat V\Big(\hat \mu_{y_j}^{PS}\Big) = 
\frac{1}{n_j} \Bigg( \sum_{k=1}^{2} w_{jk} n_{jk} \hat V\Big(\hat\mu_{y_{jk}}^{HT}\Big) + 
\sum_{k=1}^{2} (1 - w_{jk}) \frac{n_{jk}}{n_j} \hat V\Big(\hat\mu_{y_{jk}}^{HT}\Big) \Bigg)
\]
Recall that \(k\) indexes over our strata, which in this case is forested and non-forested sampled units. We also have \(w_k\), which is a survey weight entirely decided by the true proportion of eco-subsection \(j\) which is forested. For example, if eco-subsection \(j\) was 80\% forested, we would have \(w_1 = 0.8\) and \(w_2 = 0.2\). Therefore if we had under-sampled forests at only 60\% of our samples, we correct this under-sampling with our survey weights and this results in a better estimate of \(\mu_{y_j}\) that is less variable.

The post-stratified estimator is a great alternative to the Horvitz-Thompson estimator when auxiliary information related to the response variable of interest is available, and in that situation there is not a justifiable reason to pick the Horvitz-Thompson estimator over the post-stratified estimator as the direct estimator of choice. We have access to the information needed to compute the post-stratified estimate in the Interior West so we will primarily be comparing our indirect estimators to the post-stratified estimator in our results. Also, our area-level indirect model-based estimators will be based on the post-stratified estimate.

\hypertarget{the-survey-regression-estimator}{%
\subsection{The Survey Regression Estimator}\label{the-survey-regression-estimator}}

Another direct estimator that is often used for small area estimation is the survey regression estimator. While we do not implement in this estimator, the unit-level frequentist EBLUP estimator can be written as a weighted average of the survey regression estimator and a regression-synthetic estimator. Thus, it is important to understand the intuition behind this estimator in order to fully understand how the unit-level frequentist EBLUP estimator works. We can represent the survey regression estimator as follows:
\begin{align}
\hat \mu_{y_j}^{SR} = \frac{1}{N_j} \sum_{i \in U_j} \hat y_{ji} +
\frac{1}{n_j} \sum_{i \in s_j} (y_{ji} - \hat y_{ji})
\end{align}
where \(N_j\) is the population size in the \(j\)th area. The survey regression estimator has a particular subtlety which makes it arguably not a direct estimator. Particularly, the \(\hat y_{j,i}\) is calculated through linear regression with \(\beta\)'s fit by ordinary least squares across the study region, rather than the small area. This is the case for this particular survey regression estimator, as it is the estimator which we can partially represent the unit-level EBLUP estimator by. One may make a quite legitimate argument that this estimator then should in fact be considered an implicit model-based estimator, however Rao (2014) sticks to the convention that this estimator is in fact still a direct estimator. He argues that we should consider this estimator to be a direct estimator as the strength we are borrowing does not increase the ``effective'' sample size. Making a final decision on this estimator's class is left as an exercise to the reader.

\hypertarget{implicit-model-based-indirect-estimation}{%
\section{Implicit Model-Based Indirect Estimation}\label{implicit-model-based-indirect-estimation}}

Often times it is the case that direct estimation techniques do not provide sufficiently small standard errors to be able to make informative inferences at the small area level. That is the case in our situation, as the FIA's survey design was based around making inferences at larger levels than eco-subsections. One relatively simple approach to decreasing variance of estimates at a small area level is to use auxiliary data from surrounding areas to help attain the estimate in the small area of interest. This process is called borrowing strength. It is very similar to the borrowing of strength outlined in Figures \ref{fig:hbu-diagram} and \ref{fig:hba-diagram}, however, in the case of implicit model-based indirect estimation we do not allow for between-area variation of explanatory variables. That is, we may borrow strength across small areas by using auxiliary data, but this auxiliary data must be aggregated and have the same effect on each small area within a designated area (in our case, this is the ecological province).

We introduce two implicit model-based indirect estimators commonly used when auxiliary data is available at the unit- and area-levels. These estimators are not used in the final analyses done in this thesis. However, they are crucial building blocks for understanding the frequentist EBLUP models we discuss and implement. We discuss the regression-synthetic estimator used when area-level auxiliary data is available and then the regression-synthetic estimator used for unit-level auxiliary data.

\hypertarget{the-area-level-regression-synthetic-estimator}{%
\subsection{The Area-level Regression-Synthetic Estimator}\label{the-area-level-regression-synthetic-estimator}}

A common method of an implicit model-based indirect estimator outlined by Rao (2014) for area-level is the regression-synthetic estimator. Here, in order to estimate our parameter of interest, \(\mu_{y_j}\), we fit an ordinary least squares linear regression on our auxiliary variable. Notably, we use \(\bar X_j\) to denote the auxiliary data population mean in the \(j\)th small area.
\begin{align}
\hat\mu_{y_j}^{RSA} = \hat\beta_0 + \hat\beta_1 \bar X_{j}
\end{align}
We note that the \(\hat \beta\)'s are calculated through standard methods of ordinary least squares. Further, it is important to recognize that in the case of the regression-synthetic estimator each small area receives the same \(\hat\beta\)'s as they are calculated by fitting an ordinary least squares model over all small areas within their eco-province. This is the realization of the requirement that the auxiliary information must have the same effect on each small area. It is also important to note that this estimator can be generalized to \(p\) predictors, however in this thesis we use \(p=1\).

\hypertarget{the-unit-level-regression-synthetic-estimator}{%
\subsection{The Unit-level Regression-Synthetic Estimator}\label{the-unit-level-regression-synthetic-estimator}}

The unit-level regression-synthetic estimator is very similar to the area-level regression-synthetic estimator. Rather than the ordinary least squares model being fit at the area-level, it is fit at the unit-level. We first fit this unit-level model:
\[
\hat y_{ji} = \hat\beta_0 + \hat\beta_1 X_{ji}
\]
Note that we use capital \(X\) to denote the population mean of our explanatory variable at the unit-level. We can do express the unit-level regression-synthetic estimator below:
\begin{align}
\hat\mu_{y_j}^{RSU} = \frac{1}{n_j} \sum_{i \in s_j} \hat y_{ji}
\end{align}
This estimator is very similar to the area-level regression-synthetic estimator as it fits an ordinarily least squares model over the eco-province our small area is in, and the auxiliary information has the same effect on each small area.

\hypertarget{explicit-model-based-indirect-estimation}{%
\section{Explicit Model-Based Indirect Estimation}\label{explicit-model-based-indirect-estimation}}

Often times, implicit model-based indirect estimators still do not reduce variance enough to give us estimates with a reasonably low variance. However, we can turn to explicit model-based indirect estimation in order to combat this issue further. Explicit model-based indirect estimators are extremely useful when relevant auxiliary data is available and we would like to allow for between-area variation of these auxiliary variables. By allowing for this between-area variation our models should fit to the data better given that there is truly between-area variation in the population. In the case of forests across a large portion of the United States, this assumption is very reasonable and one could attribute this variation to a number of factors such as temperature, humidity, or even elevation. Rather than attempting the daunting and nonsensical task of collecting data to fully explain this between-area variation, we fit a mixed model.

In small area estimation it is most common that the mixed model estimator used is one of the EBLUP estimators: either the unit- or area-level variant depending on the resolution of auxiliary data preferred. The EBLUP estimators are unbiased given the modeling assumptions are met, similar to the post-stratified estimator. We will now explore the model specifications of these EBLUP estimators at the unit- and area-level.

\hypertarget{the-unit-level-eblup}{%
\subsection{The Unit-level EBLUP}\label{the-unit-level-eblup}}

In the case of the unit-level EBLUP, we first fit a varying-intercepts linear mixed model at the unit-level:
\begin{align}
y_{i,j} = x_{ij} \beta + \nu_j + e_{ij} \label{eq:unit-mod}
\end{align}
where \(\beta\) is calculated from data across the eco-province and will stay constant for each eco-subsection. However, \(\nu_j\), the ``random effect,'' will be different for each eco-subsection. This random effect helps account for variation between eco-subsections that the auxiliary data does not account for. The random effects \(\nu\) and sampling errors \(e\) are assumed to be distributed as follows:
\[
 \newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{s}}}{~}}
\nu_j \stackrel{\text{iid}}{\sim} \text{N}(0,~ \hat\sigma^2_{\nu}), \quad
e_{ij} \stackrel{\text{ind}}{\sim}\text{N}(0,~\hat\sigma^2_e).
\]
Intuitively, we can think of the variance of the random effect, \(\hat\sigma^2_\nu\), as an estimate of the heterogeneity between our small areas \(j\) after we account for our fixed effects (Breidenbach \& Astrup, 2012). We can also think of the estimate of the variance for the sampling errors as a measurement of within-area variability. From our mixed model, Rao (2014) derives the unit-level EBLUP estimator fit in the frequentist paradigm as the weighted average of the survey regression estimator and the regression-synthetic estimator:
\begin{align}
\hat \mu_{y_j}^{FRU} = \hat \gamma_j \hat\mu_{y_j}^{SR} + (1 - \hat \gamma_j) \hat\mu_{y_j}^{RSU}
\end{align}
where
\begin{align}
\hat\gamma_j = \frac{\hat\sigma^2_\nu}{\hat\sigma^2_\nu + \hat\sigma^2_e}
\end{align}
It is easy to think about \(\hat\gamma_j\) if we break it down into two parts: it's numerator and denominator. The numerator measures the between-area variation, and the denominator measures the between-area variation plus the within-area variation, i.e.~total variability. Thus, when a large proportion of the overall variation is between-area variation, the unit-level EBLUP will put more weight on the survey regression estimator.

For this thesis, in order to compute the variance of the unit-level EBLUP estimator we chose to bootstrap. Bootstrapping is the process taking repeated samples of our data with replacement, computing our statistic on that new bootstrap sample, and then repeating this process many times (Efron, 1992). The property of the bootstrap which is useful to us is that it produces good estimates for the variance of our estimator. For our application, we bootstrap within each small area to compute the variance. For each eco-subsection \(j\), we sample \(n_j\) plots with replacement. Next, we fit the unit-level EBLUP estimator to the bootstrapped eco-subsections and produce our mean estimate on our first bootstrap sample (we denote each bootstrap sample with \(b\), so in this case \(b=1\)).
\[
\hat\mu_j^{FRU}(b=1)
\]
Next, we repeat this sampling \(B\) times, and in our case we choose \(B = 500\) to produce 500 estimates:
\[
\hat\mu_j^{FRU}(b), \quad b = 1,\dots,500
\]
Finally, we compute our estimate of the variance:
\begin{align}
\hat V(\hat\mu_j) = \frac{1}{B-1} \sum_{b=1}^{B}\Big(\hat\mu^{FRU}_j(b) - \overline\mu_j^B\Big)^2
\end{align}
where \(\overline\mu_j^B\) denotes the sample mean of the bootstrap estimates of \(\mu_j\).

\hypertarget{the-area-level-eblup}{%
\subsection{The Area-level EBLUP}\label{the-area-level-eblup}}

We can also fit the EBLUP at the area-level. In order to do this, we fit the following linear mixed model:
\begin{align}
\hat\mu_{j}^{PS} = x_j \beta + \nu_j + e_j \label{eq:area-mod}
\end{align}
where we assume
\[
 \newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{s}}}{~}}
\nu_j \stackrel{\text{iid}}{\sim} \text{N}(0,~ \hat\sigma^2_{\nu}), \quad e_{j} \stackrel{\text{ind}}{\sim}\text{N}(0, \sigma^2_j).
\]
In the case of the area-level EBLUP, the within-area variation, \(\sigma^2_j\) is assumed to be known and is not a quantity we estimate. However, we still estimate the between-area variation, \(\hat\sigma_\nu^2\). The area-level EBLUP estimator is expressed by Rao (2014) as a weighted average of the direct estimator and the regression-synthetic estimator:
\begin{align}
\hat \mu_j^{FRA} = \hat\gamma_j \hat\mu_j^{PS} + (1 - \hat\gamma_j) \hat\mu_j^{RSA} \label{eq:eblup-area-weight}
\end{align}
where
\begin{align}
\hat \gamma_j = \frac{\hat \sigma_\nu^2}{\sigma^2_j + \hat\sigma_\nu^2}
\end{align}
Again, \(\hat\gamma\) is the ratio of between-area variation and total variation. We note that when most of the variation is between-area variation, the area-level EBLUP will rely on the post-stratified estimator and otherwise it will rely more heavily on the area-level regression-synthetic estimator. We compute the variance of this estimator by using the MSE of this estimator. The MSE is expressed by the following equation (Hidiroglou \& You, 2016):
\begin{align}
\hat V(\hat\mu_j^{FRA}) = MSE(\hat\mu_j^{FRA}) = g_{1j} + g_{2j} + 2g_{3j}
\end{align}
where
\begin{align*}
&g_{1j} = \hat\gamma_j \sigma^2_j, \\
&g_{2j} = \hat\sigma^2_\nu (1 - \hat\gamma_j)^2 \mathbf{z_j'} \Big(\sum_{j} \hat\gamma_j \mathbf{z_j} \mathbf{z_j'}\Big)^{-1} \mathbf{z_j}, \\
&g_{3j} = (\sigma^2_j)^2 \cdot (\hat\sigma^2_\nu + \sigma^2_j)^{-3} \cdot 2(\sum_{j} (\hat\sigma^2_\nu + \sigma^2_j)^{-2})^{-1}
\end{align*}
where
\[
\mathbf{z_j} = \begin{bmatrix}
1 \\ \overline X_j
\end{bmatrix}
\]
We can intuitively think about each \(g_{\#j}\) as follows: \(g_{1j}\) accounts of for within-area variation, \(g_{2j}\) accounts for variation in estimating the regression parameter \(\beta\), and \(g_{3j}\) accounts for model variance estimation (Hidiroglou \& You, 2016).

\hypertarget{a-hierarchical-bayesian-approach}{%
\section{A Hierarchical Bayesian Approach}\label{a-hierarchical-bayesian-approach}}

So far in this chapter, we have explored common frequentist approaches to small area estimation. However, the research goal of this thesis is to study the performance of the hierarchical Bayesian model for small area estimation. With a hierarchical Bayesian model, we derive the posterior distribution of our variable of interest with either Markov Chain Monte Carlo (MCMC) methods or through numerical integration. We do this by considering both the likelihood (data given unknown parameters) and prior distributions. This allows us to use Bayes' Theorem in order to attain our posterior.

Before introducing a slew of notation regarding Bayesian statistics, let's take a step back to recall the type of questions we have been asking so far. Each estimator in this chapter so far has attempted to quantify the mean of our response variable \(y\) in each eco-subsection \(j\), that is estimating \(\mu_{y_j}\). The way in which we attain our estimate \(\hat\mu_{y_j}\) and its variance is by asking the question: ``What sort of \(\hat\mu_{y_j}\) would we expect to get under hypothetical resampling?'' This question is the basis of the name ``frequentist'' as our estimates in this frequentist paradigm rely on the hypothetical repeated (frequent) sampling. When we choose to use Bayesian methods to estimate \(\mu_{y_j}\), we ask a different question: ``What is our knowledge of \(\mu_{y_j}\) based on the data and prior information?'' (Bray, 2020).

These questions bring up a philosophical difference between Bayesian and frequentist statistics about the addition of prior information. One may object to this, however in this thesis we add very little prior information and rely quite heavily on the data to much of the work for us. Secondly, the addition of prior information allows us to attain a posterior distribution for our estimate of \(\mu_{y_j}\), rather than a point estimate. Now with this philosophical difference in mind, recall Bayes' Theorem below:
\begin{align}
P(A ~\vert~ B) &= \frac{P(B ~\vert~ A) \cdot P(A)}{P(B)} \\
&\propto P(B ~\vert~ A) \cdot P(A)
\end{align}
Bayes' Theorem allows us to derive one conditional probability, \(P(A ~\vert~ B)\), given that we know the inverse conditional probability and each marginal distribution (Bayes, 1763). This theorem is the basis of our hierarchical Bayesian models. Replacing \(A\) with our parameter of interest, \(\mu_{y_j}\), and \(B\) with our data we get the following equation:
\begin{align}
f(\mu_{y_j} ~\vert~ \text{data}) \propto P(\text{data} ~\vert~ \mu_{y_j}) \cdot P(\mu_{y_j})
\label{eq:bayes-thm}
\end{align}
In our application, we are interested in making inferences about \(\mu_{y_j}\) given the data we have. This is represented on the left hand side of Equation \eqref{eq:bayes-thm}. We call this quantity the posterior distribution of \(\mu_{y_j}\) given the data. As we continue to read left to right, we have our likelihood (or, data) multiplied with the distribution of \(\mu_{y_j}\). Clearly we do not know the true value of \(\mu_{y_j}\) or else we would not attempt to estimate \(\mu_{y_j}\). In order to estimate our posterior distribution we must multiply something with the likelihood function, so we choose a ``prior'' distribution which reflects our prior belief of the distribution of \(\mu_{y_j}\). We can visualize obtaining a posterior distribution below:
\clearpage
\begin{figure}
\includegraphics{thesis_files/figure-latex/unnamed-chunk-11-1} \caption[Prior, likelihood, and posterior distributions.]{Hypothetical prior, likelihood, and posterior distributions. The posterior is inbetween the prior and likelihood as it is the normalized product of these distributions.}\label{fig:unnamed-chunk-11}
\end{figure}
Here we can see that our estimate is informed by both the data and the prior distribution, rather than in the frequentist paradigm where only the data is used. We also see that if the prior is ``flat,'' our posterior would just be our likelihood function, leading us to the same result as we achieve through a frequentist analysis, just though a different method.

These are the concepts and basics of Bayesian statistics applied to the problem at hand: estimating \(\mu_{y_j}\). We introduce two methods which estimate \(\hat\mu_{y_j}\): the unit-level hierarchical Bayesian estimator and the area-level hierarchical Bayesian estimator. These estimators are analogous to the unit-level EBLUP and the area-level EBLUP, respectively. Notably, when our variance parameters are known and we use flat priors to attain our posterior the hierarchical Bayesian approach and EBLUPs give the same estimates and measures of variability (Rao, 2014). However, we of course do not know the underlying parameter values of our variances. We now introduce each estimator.

\hypertarget{the-unit-level-hierarchical-bayesian-estimator}{%
\subsection{The Unit-level Hierarchical Bayesian Estimator}\label{the-unit-level-hierarchical-bayesian-estimator}}

For the unit-level hierarchical Bayesian estimator, we start with Equation \eqref{eq:unit-mod} as we did for the unit-level frequentist EBLUP. We now just apply a hierarchical Bayesian approach to this equation. We must assume a prior distribution on \(\beta\), \(\sigma^2_\nu\), and \(\sigma^2_e\). We now specify Equation \eqref{eq:unit-mod} as a hierarchical Bayesian model:
\begin{align}
&y_{ij} ~\vert~ \beta,~ \nu_j, \sigma^2_e \sim \text{N}(x_{ij}\beta + \nu_j,~ \sigma^2_e), \nonumber \\
&\nu_j ~\vert~ \sigma^2_\nu \sim \text{N}(0,~ \sigma^2_\nu),\nonumber \\
&f(\beta,~ \sigma^2_\nu, ~\sigma^2_e) = f(\beta)\cdot f(\sigma^2_\nu) \cdot f(\sigma^2_e).
\end{align}
where \(f(\beta)\), \(f(\sigma^2_\nu)\), and \(f(\sigma^2_e)\) are priors specified below:
\begin{align*}
&f(\beta) \propto 1, \\
&\frac{f(\sigma^2_e)}{f(\sigma^2_\nu)} = \text{IG}(\text{df} = 0,~ \text{scale} = 0).
\end{align*}
Now that we have specified the estimator, it is easy to attain the small area estimate and variance. For the estimate we have the posterior distribution of \(\mu_{y_j}\) given :
\begin{align}
\hat\mu_{y_j}^{HBU} = E[\mu_{y_j} ~\vert~ \mathbf{y_j}]
\end{align}
where \(\mathbf{y_j}\) is the vector of response values for sampled units in the \(j\)th small area. Finally, for the variance we have:
\begin{align}
\hat V(\hat \mu_{y_j}^{HBU}) = V(\mu_{y_j} ~\vert~ \mathbf{y_j}).
\end{align}
\hypertarget{the-area-level-hierarchical-bayesian-estimator}{%
\subsection{The Area-level Hierarchical Bayesian Estimator}\label{the-area-level-hierarchical-bayesian-estimator}}

For the area-level hierarchical Bayesian estimator, we start with Equation \eqref{eq:area-mod} as we did for the area-level frequentist EBLUP. We now just apply a hierarchical Bayesian approach to this equation. We must assume a prior distribution on \(\beta\) and \(\sigma^2_\nu\). We now specify Equation \eqref{eq:area-mod} as a hierarchical Bayesian model:
\begin{align}
&\hat\mu_{y_j}^{PS} ~\vert~ \mu_{y_j},~ \beta,~ \sigma^2_\nu \sim \text{N}(\mu_{y_j},~ \psi_j), \nonumber \\
&\mu_{y_j} ~\vert~ \beta,~ \sigma^2_\nu \sim \text{N}(x_j\beta,~ b_j^2 \sigma^2_\nu),\nonumber \\
&f(\beta,~ \sigma^2_\nu) = f(\beta)\cdot f(\sigma^2_\nu).
\end{align}
where \(f(\beta)\) and \(f(\sigma^2_\nu)\) and \(f(\sigma^2_e)\) are priors specified below:
\begin{align*}
&f(\beta) \propto 1, \\
&f(\sigma^2_\nu) = \text{IG}(\text{df} = 10000 \cdot m,~ \text{scale} = 1).
\end{align*}
where \(m\) is the number of small areas in the eco-province we fit the estimator in. Now that we have specified the estimator, it is easy to attain the small area estimate and variance. We attain these estimates through numerical integration. For the estimate in eco-subsection \(j\) we use the expected value of the posterior distribution of \(\mu_{y_j}\):
\begin{align}
\hat\mu_{y_j}^{HBA} = E[\mu_{y_j} ~\vert~ \hat\mu_{y_j}^{PS}] 
\end{align}
And for the variance in eco-subsection \(j\) we have the variance of the same posterior distribution:
\begin{align}
\hat V(\hat\mu_{y_j}^{HBA}) = V(\mu_{y_j} ~\vert~ \hat\mu_{y_j}^{PS})
\end{align}
\hypertarget{results}{%
\chapter{Results}\label{results}}

This chapter addresses the performance of the six estimators in this thesis: the sample mean, the post-stratified estimator, the unit-level EBLUP, the area-level EBLUP, the unit-level hierarchical Bayesian estimator, and the area-level hierarchical Bayesian estimator.

We used the R statistical software to compute our results (R Core Team, 2020). The sample mean was computed using the \emph{sae} (Molina \& Marhuenda, 2015). The post-stratified estimates were computed using \emph{mase} (K. McConville, Tang, Zhu, Cheung, \& Li, 2018). The frequentist EBLUP estimates were computed using \emph{sae} (Molina \& Marhuenda, 2015). The hierarchical Bayesian estimates were computed using \emph{hbsae} (Boonstra, 2012). The results were tidied, processed, and visualized using many \emph{tidyverse} packages (Wickham et al., 2019). Our data spans the entire Interior West, however we were forced to exclude a few eco-provinces. Namely two eco-provinces with a very small number of eco-subsections which caused errors in producing the unit-level hierarchical Bayesian estimates. We also had to not include estimates in a few eco-subsections with either none or very close to no sampled areas with non-zero values for our variables of interest. That is, areas which are in extremely non-forested areas. While this is disappointing, we still were able to fit these estimators to the great majority of our data.

This thesis uses all six of these estimators to produce estimates for basal area (square-foot), tree count per acre, above-ground biomass (lbs), and net volume (\(\text{ft}^3\)). The EBLUP and hierarchical Bayesian estimators use one explanatory variable, total canopy cover, to produce estimates. Our estimation occurs at the eco-subsection level, and thus we have 10,176 estimates produced (six estimators, four response variables, and 424 eco-subsections).

We are generally concerned with producing estimates that have both low variance and low bias. This chapter primarily aims to answer questions regarding these two quantities. In order to do so, we will summarize our findings both globally and by briefly examining a subset of our results in the Northern Rocky Forest. This subset examination allows us to dig in to results without much aggregation or incomprehensible plots due to the large amount of results we have. We explore both metrics and visualizations of variance and bias across the interior west in order to deeply understand the performance of our estimators. This thesis is, in many ways, a study navigating the bias-variance trade-off in depth in a real world setting. While quantifying variance of estimates is a generally straightforward task, producing an accurate depiction of bias is extremely difficult without knowing population parameters.

\hypertarget{the-big-picture}{%
\section{The Big Picture}\label{the-big-picture}}

We will now investigate the performance of the six estimators used in this thesis. In order to explore variance, the primary metric we use is the coefficient of variation (\(CV\)). This metric, for each estimator, is defined as the standard deviation of our estimate divided by the sample mean in that small area (\(\hat \mu_{y_j}^{HT}\)). This allows us to normalize our variation across different response variables and areas that are more or less forested. We can express the coefficient of variation as follows:
\begin{align}
CV_{y_j} = \frac{\hat\sigma_{y_j}}{\hat\mu_{y_j}^{HT}}
\end{align}
It is notable that when the mean of the variable of interest is small, we will sometimes get strangely large coefficients of variation for estimators that borrow strength. This phenomenon can generally be thought of as more to do with intrinsic (and often very good) qualities of the estimator rather than correlation with poor performance of the estimator when the sample mean is small. Notably, estimators which borrow strength to a large degree in situations where the sample mean is low and the areas which we borrow strength from have significantly higher values will have higher variance due to this heterogeneity. We will explore this idea in depth further on in this chapter.
\clearpage
In order to visualize the coefficients of variation in an appealing way, we have filtered out all observations greater than one in the following plot:
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/cov-violin-1} 

}

\caption[Distribution of the coefficient of variation of each eastimator]{Distribution of the coefficient of variation of each eastimator. The black dot in each violin segment represents the median coefficient of variation value. The width of the violin segments corresponds to the density of points in the given value range. Values greater than 1 truncated in plot, however still considered in median calculation.}\label{fig:cov-violin}
\end{figure}
Figure \ref{fig:cov-violin} shows the coefficient of variation for each estimator. We see a slight reduction in the coefficient of variation when moving from the sample mean to post-stratification, and a decently large reduction in variation when adding auxiliary data with either EBLUP estimator. We also see that the unit-level hierarchical Bayesian estimator performs quite similarly to the unit-level EBLUP. This makes sense as at the unit-level we have so much data that it outweighs the contribution of any priors significantly. Finally, we see a huge reduction in the coefficient of variation compared to all other estimators when we fit the area-level hierarchical Bayesian estimator. The median coefficient of variation for this area-level hierarchical Bayesian estimator (0.055) is less than half of the next best performing median coefficient of variation from the area-level EBLUP (0.125). Examining the quantiles of these estimator's coefficient of variation continues to show the superiority of the area-level hierarchical Bayesian estimator while also demonstrating the metric's tendency to have some extremely large values:
\begin{longtable}[t]{lrrrrr}
\caption[Coefficient of Variation Quantiles]{\label{tab:quantile-table}Quantiles of Each Estimator's Coefficient of Variation}\\
\toprule
Estimator & 0\% & 25\% & 50\% & 75\% & 100\%\\
\midrule
Sample Mean & 0.038 & 0.123 & 0.189 & 0.318 & 1.000\\
Post-Stratification & 0.034 & 0.105 & 0.158 & 0.255 & 1.000\\
Area EBLUP & 0.015 & 0.087 & 0.125 & 0.213 & 0.999\\
Unit EBLUP & 0.026 & 0.085 & 0.131 & 0.244 & 52.408\\
Area HB & 0.007 & 0.034 & 0.055 & 0.112 & 56.401\\
\addlinespace
Unit HB & 0.008 & 0.078 & 0.150 & 0.486 & 299.673\\
\bottomrule
\end{longtable}
While Figure \ref{fig:cov-violin} and Table \ref{tab:quantile-table} gives us a good sense of the distribution, we must acknowledge that we truncated some values in Figure \ref{fig:cov-violin} and further investigate why the coefficient of variation tends to get very large in some cases. By examining the distribution and performance of the coefficients of variation by truncating the tails in Figure \ref{fig:cov-violin} we are able to gain insight that would be much harder to see while including the tails. Now, we can look more in depth at the tails of these distributions. Three estimators had coefficients of variation that exceeded 1. The below table shows the count and proportion of each:
\begin{longtable}[t]{lrr}
\caption{\label{tab:over-one}Coefficient of variation estimates greater than one}\\
\toprule
Estimator & Count & Proportion\\
\midrule
Post-Stratification & 0 & 0.000\\
Area EBLUP & 0 & 0.000\\
Sample Mean & 0 & 0.000\\
Area HB & 81 & 0.048\\
Unit EBLUP & 103 & 0.061\\
\addlinespace
Unit HB & 289 & 0.170\\
\bottomrule
\end{longtable}
We see that there are three estimators with coefficient of variations greater than one. Both the frequentist and hierarchical Bayesian unit-level estimators have many values greater than one, and the area-level hierarchical Bayesian has some as well. To understand why these particular estimators have many estimates greater than one, we first need to explore the small areas which have these coefficient of variation values. To do so, we can examine the mean response value for estimates where the coefficient of variation is greater than one and when it is less than one.
\clearpage
\begin{longtable}[t]{lrr}
\caption{\label{tab:covg1l1}Mean Estimates Where Coefficent of Variation is Greater Than and Less Than One}\\
\toprule
Response & Mean Estimate (CV < 1) & Mean Estimate (CV > 1)\\
\midrule
BALIVE\_TPA & 39.655 & 17.473\\
BIOLIVE\_TPA & 11.522 & 3.760\\
CNTLIVE\_TPA & 181.724 & 96.469\\
VOLNLIVE\_TPA & 615.601 & 130.547\\
\bottomrule
\end{longtable}
From this table we are able to confirm that we see high coefficients of variation more often in areas with lower values for our response variable. It now becomes apparent why some estimators have many coefficients of variation greater than one and others have none or very few. The three estimators which have coefficients of variation greater than one all will borrow a great deal of strength from outside eco-subsections when those outside eco-subections have a much higher response variable value than the eco-subsection of interest. In other words, with these three estimators, eco-subsections with lower response variable sample means will produce larger estimates. These larger estimates require larger a larger variance as we are unsure whether we had a non-representative sample (too low) in the given eco-subsection or if this eco-subsection truly has a low value for its response variable(s). However, when we are selecting methods of estimation which borrow strength, this is a property we like to see. In fact, estimators which do not borrow strength in these situations and have small coefficients of variation could be producing under-estimates with an artificial amount of precision. We can first visualize this phenomenon by examining the coefficient of variation of each estimator across the Interior West:
\clearpage
\begin{figure}

{\centering \includegraphics{thesis_files/figure-latex/cv-map-1} 

}

\caption[Coefficient of variation map]{Average log coefficient of variation of each estimator in each eco-subsection plotted onto a map of the Interior West. The average is taken over the four response variables. The natural log is used to perserve a reasonable color scale. Grey areas represent areas where models were not fit.}\label{fig:cv-map}
\end{figure}
We see that the three estimators which have coefficient of variation values higher than one generally produce these values in heavily deserted areas in the southern parts of the Interior West and some deserts in the north parts of Nevada. It is sensible that these areas may truly have low values for our response variables given that the post-stratified estimator is doing its job correctly and the population totals for forested and non-forested areas are correct. On the other hand, it may be reasonable to use a higher variance estimate in these cases and borrow strength from surrounding areas. While it is hard to say which approach to handling these situations gives a better estimate, it highlights distinct ways in which the hierarchical Bayesian estimators (and the unit-level EBLUP) perform differently than the direct estimators and area-level EBLUP.

\hypertarget{zooming-in-the-northern-rocky-forest}{%
\section{Zooming In: The Northern Rocky Forest}\label{zooming-in-the-northern-rocky-forest}}

In order to give ourselves a closer look at \emph{how} borrowing of strength differs between the area-level hierarchical Bayesian estimator and the area-level EBLUP estimator, we zoom in to basal area in the Northern Rocky Forest. Immediately, when we examine the Northern Rocky Forest we notice the behavior of the hierarchical Bayesian area-level estimator ``pulling up'' lower valued eco-subsections compared to their post-stratified estimate:
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-13-1} 

}

\caption[HB Area and Post-stratified estimates in M333]{The hierarchical Bayesian area-level estimator and post-stratified estimator predicting basal area in the Northern Rocky Forest. The error bars depict two standard errors above an below the estimate of our response variable.}\label{fig:unnamed-chunk-13}
\end{figure}
We also observe that the eco-subsections with high response values will get ``pulled down,'' however the coefficient of variation does not increase significantly here as the sample mean is still relatively high. The ``pulling up'' and ``pushing down'' of estimates is a trait expected of these hierarchical Bayesian models, and it is the realization of ``borrowing strength'' or ``pooling information'' (McElreath, 2020).

The frequentist area-level estimator takes a different approach to making these estimates. Notably, in the case of small within-area variation we see that the EBLUP will put a heavy weight on the post-stratified estimate from Equation \eqref{eq:eblup-area-weight}. We can see this occur by comparing the area-level frequentist estimator to the post-stratified estimate with the same eco-province and response variable as above:
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-14-1} 

}

\caption[EBLUP area-level and Post-stratified estimates in M333]{The frequentist area-level estimator and post-stratified estimator predicting basal area in the Northern Rocky Forest. The error bars depict two standard errors above an below the estimate of our response variable.}\label{fig:unnamed-chunk-14}
\end{figure}
Overall, we can see that the frequentist area-level estimator performs much more similarly to the post-stratified estimator, a trait that may or may not be appealing based on the confidence we have in representative sampling. However, it is important to note that the frequentist area-level estimator is still borrowing strength, it just seems that it will often borrow less strength than the hierarchical Bayesian area-level estimator. This is expected, if we recall Equation \eqref{eq:eblup-area-weight} we note that when a large proportion of the total variation is between-area variation, the area-level EBLUP will rely heavily on the post-stratified estimate. Thus, in eco-subsections with low within-area variation the area-level EBLUP will produce an estimate very similar to the post-stratified estimate. It is important to recall that much of our data contains many observations which equal zero (Figure \ref{fig:hists} and Table \ref{tab:var-tab}). So, in eco-subsections with a low number of sampled plots and a high proportion of sampled plots with response values equal to zero we may artificially lower our variance. The hierarchical Bayesian estimators have the unique property of considering the uncertainty in the variance estimate when producing estimates, rather than relying on just a point value.

\hypertarget{stepping-back}{%
\section{Stepping Back}\label{stepping-back}}

When we narrow our vision to the Northern Rocky Forest it does seem like the area-level hierarchical Bayesian estimator clearly outperforms the analogous frequentist estimator. However, we must return to the Interior West in order to understand trends of estimator performance over space. To do this, we simply examine the eco-subsections in which the area-level hierarchical Bayesian estimator has lower variance than the post-stratified estimator and the area-level EBLUP:
\begin{figure}

{\centering \includegraphics{thesis_files/figure-latex/unnamed-chunk-15-1} 

}

\caption[Area-level coefficient of variation comparison across the Interior West]{The area-level hierarcical Bayesian estimator's coefficient of variation compared with the coefficient of variation for the post-stratified estimator (left) and area-level EBLUP (right). Eco-subsections in seafoam green indicate areas where the coefficient of variation is lower for the area-level hierarchical Bayesian estimator, red areas indicate that the coefficient of variation of the hierarchical Bayesian estimator is greater than the estimator we are comparing it to. Grey areas indicate areas where we did not fit these estimators.}\label{fig:unnamed-chunk-15}
\end{figure}
Generally, we continue to see the trend in Figure \ref{fig:cv-map} where the area-level hierarchical Bayesian estimator producing estimates with higher variance in heavily deserted areas in the southern parts of the Interior West and deserts in the north parts of Nevada. The areas in which the area-level hierarchical Bayesian estimator has more variation than the post-stratified estimator and the area-level EBLUP are clearly non-randomly distributed across the Interior West. This gives us great insight as to where we may want to use this hierarchical Bayesian estimator and further supports our idea that the area-level hierarchical Bayesian estimator has a high coefficient of variation in non-forested areas.

We have largely discussed the variance of these estimators thus far, as we cannot know the true bias of an estimator without knowing the population parameters \(\mu_{y_j}\) in each eco-subsection. However, in an attempt to quantify the magnitude of difference between two estimators we introduce percent relative difference, which we define below.
\begin{align*}
PRD(\hat\mu_1,~ \hat\mu_2) = \frac{\hat \mu_1 - \hat\mu_2}{\hat\mu_2} \cdot 100
\end{align*}
While we could explore percent relative difference between all combinations of estimators, the Forest Inventory and Analysis Program's go-to estimator is post-stratification, so we will compare the remaining five estimators to post-stratification. Also, post-stratification, as discussed in Methods, is an unbiased estimator given assumptions about our auxiliary data. Thus, we will rely on it for some proxy of truth. However we must realize that this proxy is quite weak. Below, we can see the quantiles of percent relative difference to post-stratification.
\begin{longtable}[t]{lrrrrr}
\caption{\label{tab:prd-tab}Quantiles of Percent Relative Difference to the Post-Stratified Estimator}\\
\toprule
Estimator & 0\% & 25\% & 50\% & 75\% & 100\%\\
\midrule
Sample Mean & -60.997 & -3.826 & 0.056 & 5.610 & 1102.114\\
Unit HB & -99.928 & 1.326 & 21.816 & 94.770 & 24224.087\\
Area HB & -74.491 & -13.192 & 13.095 & 66.254 & 105994.261\\
Unit EBLUP & -80.319 & 3.521 & 25.471 & 99.885 & 27724.471\\
Area EBLUP & -79.641 & -3.747 & -0.165 & 2.802 & 160.843\\
\bottomrule
\end{longtable}
These quantiles continue to enforce the idea that the area-level EBLUP generally performs very similarly to the post-stratified estimator (albeit with lower variance). Notably though, the percent relative difference between the area-level hierarchical Bayesian estimator and the post-stratified estimator is small compared to both unit-level models. The median values in Table \ref{tab:prd-tab} can be thought of some sort of proxy for each estimator's bias, but this is a difficult equivalency to make when we do not have the parameter values. While this proxy for bias may be weak, it is reassuring that for the area-level hierarchical Bayesian estimator our median (50\% quantile) percent relative difference is somewhat low compared to other common estimators.

Another aspect of our results that percent relative difference allows us to investigate is the performance of an estimator which we have hardly addressed so far: the hierarchical Bayesian unit-level model. This thesis claims to be a hierarchical Bayesian approach to small area estimation, yet the performance of one of the hierarchical Bayesian estimators has yet to be discussed in depth in this chapter. This is because the area-level hierarchical Bayesian estimator outperforms it by a huge margin in terms of variance. However, the case of the unit-level hierarchical Bayesian estimator is interesting in itself, even if it does not increase precision of estimates. To understand its performance, let's examine its percent relative difference to the unit-level EBLUP estimator:
\begin{longtable}[t]{lrrrrr}
\caption{\label{tab:prd-unit}Quantiles of Percent Relative Difference to the Unit-level EBLUP}\\
\toprule
Estimator & 0\% & 25\% & 50\% & 75\% & 100\%\\
\midrule
Unit HB & -99.845 & -2.736 & -0.658 & -0.119 & 4.844\\
\bottomrule
\end{longtable}
\clearpage

We immediately see that the unit-level hierarchical Bayesian estimator is extremely similar to the unit-level EBLUP. That is, many of their estimates are within just a few percent difference compared to the unit-level EBLUP. This is because the data (or, likelihood) outweighs any prior information we gave the this model to a large degree. At the unit level we have so many observations that any reasonable prior has a negligible effect on the estimator's results. This illustrates the idea that under ``flat'' or ``uninformative'' priors that the hierarchical Bayesian estimates will converge to the analogous EBLUP estimator's estimates. Also, recall that not only the estimates should converge to the same point, but the variance should as well. We can see this illustrated in Figure \ref{fig:cov-violin} and below:
\begin{figure}

{\centering \includegraphics{thesis_files/figure-latex/unnamed-chunk-16-1} 

}

\caption[Unit-level coefficient of variation comparison across the Interior West]{The unit-level hierarcical Bayesian estimator's coefficient of variation compared with the coefficient of variation for the post-stratified estimator (left) and unit-level EBLUP (right). Eco-subsections in seafoam green indicate areas where the coefficient of variation is lower for the unit-level hierarchical Bayesian estimator, red areas indicate that the coefficient of variation of the hierarchical Bayesian estimator is greater than the estimator we are comparing it to. Grey areas indicate areas where we did not fit these estimators.}\label{fig:unnamed-chunk-16}
\end{figure}
The distribution of areas where the hierarchical Bayesian unit-level estimator has lower variance than the unit-level EBLUP appears to be very random. Just under 46\% of areas have the hierarchical Bayesian unit-level model with lower variance than the unit-level EBLUP. We are not making any significant variance gains by implementing the unit-level hierarchical Bayesian estimator, however there are benefits of the implementation of this estimator. First, we now have a case study of a hierarchical Bayesian estimator converging in variance and estimates to the analogous frequentist model. This allows us to verify a statement often made in Bayesian statistics courses and textbooks with a real world example rather than a mathematical proof. Second, we now have access to a much more flexible estimator than the unit-level EBLUP that does a very similar job to the unit-level EBLUP. We can ``tweak'' the behavior of this estimator through the use of priors, or even a different linking model if needed. These changes could allow for a more successful estimator at the unit-level without changes many processes computationally. If we were to attempt to approach improving the unit-level model starting with the EBLUP we would be met with restrictions due to the rigidity of the EBLUP's mathematics, and we would have to construct a new estimator entirely.

\hypertarget{discussion}{%
\chapter{Discussion}\label{discussion}}

We have introduced, fit, and presented the results of six small area estimators across the Interior West with four forestry-related response variables. Particularly, we introduce two estimators which are uncommon in forestry research: the hierarchical Bayesian unit- and area-level estimators. The area-level hierarchical Bayesian estimator reduces variance in estimates significantly compared to common direct estimators, frequentist model-based approaches (EBLUPs), and the unit-level hierarchical Bayesian estimator. We see reduction in variance compared to the area-level frequentist estimator and post-stratification in the great majority of small areas when implementing the hierarchical Bayesian area-level estimator. In extremely non-forested small areas, the hierarchical Bayesian area-level estimator consistently has higher variance than post-stratification or the area-level EBLUP. This is likely due to the way in which the area-level hierarchical Bayesian estimator borrows strength.

\hypertarget{possible-extensions}{%
\section{Possible Extensions}\label{possible-extensions}}

Now we turn to possible extensions to the work which has been done in this thesis. We explore ideas for further research on adjacent topics and possibilities for applying these methods in other locations.

First, attempting to understand the performance of these hierarchical Bayesian estimators outside of the Interior West region of the United States through implementing these methods across the country would be insightful to the generalization of these estimators. Outside of the Interior West, the Forest Inventory and Analysis Programs has research stations in the Pacific Northwest region, Northern region, and Southern region. The hierarchical Bayesian area-level estimator is also a great candidate for estimation in Alaska. The reasons for its great candidacy are twofold: first, it is expensive and difficult to send foresters to collect data in Alaska due to the extreme rurality of the area; second, Alaska is very forested and we thus would likely see a large reduction in variance of estimates when using the hierarchical Bayesian area-level estimator. The area-level hierarchical Bayesian estimator reduced variance in forested areas quite well, and applying this technique to data in Alaska could help mitigate our smaller sample size.

Another possibility for extending the research done in this thesis would be to adapt these hierarchical Bayesian estimators to produce estimates with lower variance under a variety of conditions. First, an implementation of a hybrid estimator could mitigate high variance in heavily non-forested areas. A hybrid estimator including both area-level model-based estimators which relies on the EBLUP's estimates in less-forested areas and the hierarchical Bayesian's estimates in more forested areas might result in lower variance across the board. The small areas in which the area-level hierarchical Bayesian estimator had higher variance than the area-level EBLUP were clearly non-random. A further analysis of these areas could lead to an insightful weighting system for this hybrid estimator. Another approach to mitigating the high
\begin{itemize}
\tightlist
\item
  explicit spatial analysis\\
\item
  simulation study to understand the bias of these estimators\\
\item
  different regions of the united states: list all FIA regions\\
\item
  improved software for bayesian sae, more flexible software, non-normal response? (gamma or tweedie)\\
\item
  different aux data, desert/non-desert could be helpful
\end{itemize}
\appendix

\hypertarget{code-appendix}{%
\chapter{Code Appendix}\label{code-appendix}}

This appendix includes the code used to create the estimates used in this thesis. We include both the helper functions created to run the analyses and the implementation of these helper functions.

\hypertarget{helper-functions}{%
\section{Helper Functions}\label{helper-functions}}

\hypertarget{the-sample-mean}{%
\subsection{The Sample Mean}\label{the-sample-mean}}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{direct\_estimate }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, response, small\_area) \{}
  \CommentTok{\# Load packages}
  \FunctionTok{library}\NormalTok{(sae)}
  
  \CommentTok{\# Create dataframe}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ data[[response]],}
    \AttributeTok{small\_area =}\NormalTok{ data[[small\_area]]}
\NormalTok{  )}
  
  \CommentTok{\# Compute estimate}
\NormalTok{  sae}\SpecialCharTok{::}\FunctionTok{direct}\NormalTok{(}\AttributeTok{y =}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{y,}
              \AttributeTok{dom =}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{small\_area,}
              \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{post-stratification}{%
\subsection{Post-Stratification}\label{post-stratification}}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{postStrat2\_bio }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, strata) \{}
\NormalTok{  est }\OtherTok{\textless{}{-}}\NormalTok{ mase}\SpecialCharTok{::}\FunctionTok{postStrat}\NormalTok{(}\AttributeTok{y =}\NormalTok{ data[[}\StringTok{"BIOLIVE\_TPA"}\NormalTok{]],}
                  \AttributeTok{x\_sample =}\NormalTok{ data[[}\StringTok{"FIAstrat"}\NormalTok{]],}
                  \AttributeTok{x\_pop =}\NormalTok{ strata,}
                  \AttributeTok{data\_type =} \StringTok{"totals"}\NormalTok{,}
                  \AttributeTok{var\_est =}\NormalTok{ T)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ps\_est =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{pop\_mean,}
                    \AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(est}\SpecialCharTok{$}\NormalTok{pop\_mean\_var)))}
\NormalTok{\}}
\NormalTok{postStrat2\_ba }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, strata) \{}
\NormalTok{  est }\OtherTok{\textless{}{-}}\NormalTok{ mase}\SpecialCharTok{::}\FunctionTok{postStrat}\NormalTok{(}\AttributeTok{y =}\NormalTok{ data[[}\StringTok{"BALIVE\_TPA"}\NormalTok{]],}
                  \AttributeTok{x\_sample =}\NormalTok{ data[[}\StringTok{"FIAstrat"}\NormalTok{]],}
                  \AttributeTok{x\_pop =}\NormalTok{ strata,}
                  \AttributeTok{data\_type =} \StringTok{"totals"}\NormalTok{,}
                  \AttributeTok{var\_est =}\NormalTok{ T)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ps\_est =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{pop\_mean,}
                    \AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(est}\SpecialCharTok{$}\NormalTok{pop\_mean\_var)))}
\NormalTok{\}}

\NormalTok{postStrat2\_voln }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, strata) \{}
\NormalTok{  est }\OtherTok{\textless{}{-}}\NormalTok{ mase}\SpecialCharTok{::}\FunctionTok{postStrat}\NormalTok{(}\AttributeTok{y =}\NormalTok{ data[[}\StringTok{"VOLNLIVE\_TPA"}\NormalTok{]],}
                  \AttributeTok{x\_sample =}\NormalTok{ data[[}\StringTok{"FIAstrat"}\NormalTok{]],}
                  \AttributeTok{x\_pop =}\NormalTok{ strata,}
                  \AttributeTok{data\_type =} \StringTok{"totals"}\NormalTok{,}
                  \AttributeTok{var\_est =}\NormalTok{ T)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ps\_est =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{pop\_mean,}
                    \AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(est}\SpecialCharTok{$}\NormalTok{pop\_mean\_var)))}
\NormalTok{\}}

\NormalTok{postStrat2\_cnt }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, strata) \{}
\NormalTok{  est }\OtherTok{\textless{}{-}}\NormalTok{ mase}\SpecialCharTok{::}\FunctionTok{postStrat}\NormalTok{(}\AttributeTok{y =}\NormalTok{ data[[}\StringTok{"CNTLIVE\_TPA"}\NormalTok{]],}
                  \AttributeTok{x\_sample =}\NormalTok{ data[[}\StringTok{"FIAstrat"}\NormalTok{]],}
                  \AttributeTok{x\_pop =}\NormalTok{ strata,}
                  \AttributeTok{data\_type =} \StringTok{"totals"}\NormalTok{,}
                  \AttributeTok{var\_est =}\NormalTok{ T)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ps\_est =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{pop\_mean,}
                    \AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(est}\SpecialCharTok{$}\NormalTok{pop\_mean\_var)))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{hierarchical-bayesian-unit-level}{%
\subsection{Hierarchical Bayesian Unit-Level}\label{hierarchical-bayesian-unit-level}}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hb\_unit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, formula, small\_area, pop\_data) \{}
  \CommentTok{\# Load packages}
  \FunctionTok{library}\NormalTok{(tidyverse)}
  \FunctionTok{library}\NormalTok{(hbsae)}
  
  \CommentTok{\# Create model frame}
\NormalTok{  model\_frame }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(formula, data) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{small\_area =}\NormalTok{ data[[small\_area]])}
  \FunctionTok{colnames}\NormalTok{(model\_frame) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"x"}\NormalTok{, }\StringTok{"small\_area"}\NormalTok{)}
  
  \CommentTok{\# Area population sizes}
\NormalTok{  pop\_size }\OtherTok{\textless{}{-}}\NormalTok{ pop\_data }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(zoneid }\SpecialCharTok{\%in\%}\NormalTok{ model\_frame}\SpecialCharTok{$}\NormalTok{small\_area) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(zoneid, sum) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(}\AttributeTok{pop\_size =}\NormalTok{ sum) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(pop\_size)}
  
  \CommentTok{\# Create population means matrix}
\NormalTok{  pop\_means }\OtherTok{\textless{}{-}}\NormalTok{ pop\_data }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(zoneid }\SpecialCharTok{\%in\%}\NormalTok{ model\_frame}\SpecialCharTok{$}\NormalTok{small\_area) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(zoneid, mean) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mean) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{column\_to\_rownames}\NormalTok{(}\StringTok{"zoneid"}\NormalTok{)}
  
  \CommentTok{\# Create lambda}
\NormalTok{  anova }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ small\_area, }\AttributeTok{data =}\NormalTok{ model\_frame)}
\NormalTok{  l }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(anova)[[}\DecValTok{1}\NormalTok{]][}\StringTok{"small\_area"}\NormalTok{, }\StringTok{"F value"}\NormalTok{]}
  
  \CommentTok{\# Fit the model}
\NormalTok{  mod }\OtherTok{\textless{}{-}} \FunctionTok{fSAE.Unit}\NormalTok{(}
    \AttributeTok{y =} \FunctionTok{model.frame}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ data)[, }\DecValTok{1}\NormalTok{],}
    \AttributeTok{X =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{X =} \FunctionTok{model.frame}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ data)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]),}
    \AttributeTok{area =}\NormalTok{ data[[small\_area]],}
    \AttributeTok{Narea =}\NormalTok{ pop\_size}\SpecialCharTok{$}\NormalTok{pop\_size,}
    \AttributeTok{Xpop =}\NormalTok{ pop\_means,}
    \AttributeTok{fpc =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{lambda0 =}\NormalTok{ l,}
    \AttributeTok{silent =}\NormalTok{ T}
\NormalTok{  )}

  \CommentTok{\# Calculate CoV}
\NormalTok{  mean\_y }\OtherTok{\textless{}{-}}\NormalTok{ model\_frame }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(small\_area) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_y =} \FunctionTok{mean}\NormalTok{(y))}
\NormalTok{  CoV }\OtherTok{\textless{}{-}}\NormalTok{ hbsae}\SpecialCharTok{::}\FunctionTok{SE}\NormalTok{(mod) }\SpecialCharTok{/}\NormalTok{ mean\_y}\SpecialCharTok{$}\NormalTok{mean\_y}

  \DocumentationTok{\#\# Add to model object}
\NormalTok{  mod}\SpecialCharTok{$}\NormalTok{CoV }\OtherTok{\textless{}{-}}\NormalTok{ CoV}

  \CommentTok{\# Print model}
\NormalTok{  mod}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{hierarchical-bayesian-area-level}{%
\subsection{Hierarchical Bayesian Area-Level}\label{hierarchical-bayesian-area-level}}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hb\_area }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, formula, small\_area,}
\NormalTok{                    pop\_data, post\_strat\_data) \{}
  \CommentTok{\# Load packages}
  \FunctionTok{library}\NormalTok{(tidyverse)}
  \FunctionTok{library}\NormalTok{(hbsae)}
  
  \CommentTok{\# Create unnamed model frame (to call correct y var in a filter)}
\NormalTok{  mf }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(formula, data)}
  
  \CommentTok{\# Create model frame}
\NormalTok{  model\_frame }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(formula, data) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{small\_area =}\NormalTok{ data[[small\_area]])}
  \FunctionTok{colnames}\NormalTok{(model\_frame) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"x"}\NormalTok{, }\StringTok{"small\_area"}\NormalTok{)}
  
  \CommentTok{\# Direct X}
\NormalTok{  X }\OtherTok{\textless{}{-}}\NormalTok{ pop\_data }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(zoneid }\SpecialCharTok{\%in\%}\NormalTok{ model\_frame}\SpecialCharTok{$}\NormalTok{small\_area) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(zoneid, mean) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(}\AttributeTok{mean\_x =}\NormalTok{ mean,}
                  \AttributeTok{small\_area =}\NormalTok{ zoneid) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(small\_area)}

  \CommentTok{\# Compute direct estimate}
\NormalTok{  mean }\OtherTok{\textless{}{-}} \FunctionTok{direct\_estimate}\NormalTok{(model\_frame, }\StringTok{"y"}\NormalTok{, }\StringTok{"small\_area"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{var =}\NormalTok{ SD}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
  
\NormalTok{  dir }\OtherTok{\textless{}{-}}\NormalTok{ post\_strat\_data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(response }\SpecialCharTok{\%in\%} \FunctionTok{colnames}\NormalTok{(mf)[}\DecValTok{1}\NormalTok{],}
\NormalTok{           province }\SpecialCharTok{\%in\%} \FunctionTok{unique}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{province)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{arrange}\NormalTok{(subsection)}
  
  \CommentTok{\# Create lambda}
\NormalTok{  anova }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ small\_area, }\AttributeTok{data =}\NormalTok{ model\_frame)}
\NormalTok{  l }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(anova)[[}\DecValTok{1}\NormalTok{]][}\StringTok{"small\_area"}\NormalTok{, }\StringTok{"F value"}\NormalTok{]}

  \CommentTok{\# Fit the model}
\NormalTok{  mod }\OtherTok{\textless{}{-}} \FunctionTok{fSAE.Area}\NormalTok{(}
    \AttributeTok{est.init =}\NormalTok{ dir}\SpecialCharTok{$}\NormalTok{est,}
    \AttributeTok{var.init =}\NormalTok{ dir}\SpecialCharTok{$}\NormalTok{var,}
    \AttributeTok{X =}\NormalTok{ X }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(mean\_x),}
    \AttributeTok{lambda0 =}\NormalTok{ l}
\NormalTok{  )}

  \CommentTok{\# Calculate CoV}
\NormalTok{   CoV }\OtherTok{\textless{}{-}}\NormalTok{ hbsae}\SpecialCharTok{::}\FunctionTok{SE}\NormalTok{(mod) }\SpecialCharTok{/}\NormalTok{ mean}\SpecialCharTok{$}\NormalTok{Direct}
\NormalTok{   mod}\SpecialCharTok{$}\NormalTok{CoV }\OtherTok{\textless{}{-}}\NormalTok{ CoV}

  \CommentTok{\# Print model}
\NormalTok{  mod}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{frequentist-unit-level}{%
\subsection{Frequentist Unit-Level}\label{frequentist-unit-level}}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq\_unit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, formula, small\_area, pop\_data) \{}
  \CommentTok{\# Load packages}
  \FunctionTok{library}\NormalTok{(tidyverse)}
  \FunctionTok{library}\NormalTok{(sae)}
  
  \CommentTok{\# Create model frame}
\NormalTok{  model\_frame }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(formula, data) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{small\_area =}\NormalTok{ data[[small\_area]])}
  \FunctionTok{colnames}\NormalTok{(model\_frame) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"x"}\NormalTok{, }\StringTok{"small\_area"}\NormalTok{)}
  
  \CommentTok{\# Area population sizes}
\NormalTok{  pop\_size }\OtherTok{\textless{}{-}}\NormalTok{ pop\_data }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(zoneid }\SpecialCharTok{\%in\%}\NormalTok{ model\_frame}\SpecialCharTok{$}\NormalTok{small\_area) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(zoneid, sum) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(}\AttributeTok{pop\_size =}\NormalTok{ sum,}
                  \AttributeTok{small\_area =}\NormalTok{ zoneid)}
  
  \CommentTok{\# Create population means matrix}
\NormalTok{  meanxpop }\OtherTok{\textless{}{-}}\NormalTok{ pop\_data }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(zoneid }\SpecialCharTok{\%in\%}\NormalTok{ model\_frame}\SpecialCharTok{$}\NormalTok{small\_area) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(zoneid, mean) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mean,}
                  \AttributeTok{small\_area =}\NormalTok{ zoneid)}
  
  \CommentTok{\# Fit the model}
\NormalTok{  mod }\OtherTok{\textless{}{-}} \FunctionTok{eblupBHF}\NormalTok{(}
    \AttributeTok{formula =}\NormalTok{ model\_frame}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ model\_frame}\SpecialCharTok{$}\NormalTok{x,}
    \AttributeTok{dom =}\NormalTok{ model\_frame}\SpecialCharTok{$}\NormalTok{small\_area,}
    \AttributeTok{meanxpop =}\NormalTok{ meanxpop,}
    \AttributeTok{popnsize =}\NormalTok{ pop\_size}
\NormalTok{  )}
\NormalTok{  mod}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{frequentist-area-level}{%
\subsection{Frequentist Area-Level}\label{frequentist-area-level}}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq\_area }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, formula, small\_area,}
\NormalTok{                      pop\_data, post\_strat\_data) \{}
  \CommentTok{\# Load packages}
  \FunctionTok{library}\NormalTok{(tidyverse)}
  \FunctionTok{library}\NormalTok{(sae)}
  
  \CommentTok{\# Create model frame}
\NormalTok{  model\_frame }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(formula, data) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{small\_area =}\NormalTok{ data[[small\_area]])}
  \FunctionTok{colnames}\NormalTok{(model\_frame) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"x"}\NormalTok{, }\StringTok{"small\_area"}\NormalTok{)}
\NormalTok{  model\_frame}
  
\NormalTok{  mf }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(formula, data)}
  
\NormalTok{  dir }\OtherTok{\textless{}{-}}\NormalTok{ post\_strat\_data }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{filter}\NormalTok{(response }\SpecialCharTok{\%in\%} \FunctionTok{colnames}\NormalTok{(mf)[}\DecValTok{1}\NormalTok{],}
\NormalTok{           province }\SpecialCharTok{\%in\%} \FunctionTok{unique}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{province)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{arrange}\NormalTok{(subsection)}
  
  \CommentTok{\# Direct X}
\NormalTok{  X }\OtherTok{\textless{}{-}}\NormalTok{ pop\_data }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(zoneid }\SpecialCharTok{\%in\%}\NormalTok{ model\_frame}\SpecialCharTok{$}\NormalTok{small\_area) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(zoneid, mean) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(}\AttributeTok{mean\_x =}\NormalTok{ mean,}
                  \AttributeTok{small\_area =}\NormalTok{ zoneid) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{arrange}\NormalTok{(small\_area)}
  
  \CommentTok{\# Join pop and dir}
\NormalTok{  dat }\OtherTok{\textless{}{-}}\NormalTok{ dir }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{left\_join}\NormalTok{(X, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"subsection"} \OtherTok{=} \StringTok{"small\_area"}\NormalTok{))}

  \CommentTok{\# Fit the model}
\NormalTok{  mod }\OtherTok{\textless{}{-}}\NormalTok{ sae}\SpecialCharTok{::}\FunctionTok{mseFH}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{est }\SpecialCharTok{\textasciitilde{}}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{mean\_x,}
                      \AttributeTok{vardir =}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{var)}
\NormalTok{  mod}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{coeffiicent-of-variation-functions}{%
\subsection{Coeffiicent of Variation Functions}\label{coeffiicent-of-variation-functions}}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hb\_CoV }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
  \CommentTok{\# Load packages}
  \FunctionTok{library}\NormalTok{(tidyverse)}
  \FunctionTok{library}\NormalTok{(hbsae)}
  
  \CommentTok{\# Grab CoV}
\NormalTok{  data}\SpecialCharTok{$}\NormalTok{CoV}
\NormalTok{\}}

\NormalTok{freq\_unit\_CoV }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, formula, small\_area,}
\NormalTok{                          pop\_data, }\AttributeTok{B =} \DecValTok{100}\NormalTok{) \{}
  \CommentTok{\# Load packages}
  \FunctionTok{library}\NormalTok{(tidyverse)}
  \FunctionTok{library}\NormalTok{(sae)}
  
  \CommentTok{\# Create empty items for looping}
\NormalTok{  boots }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{  fit }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{  mean\_df }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{  final }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{()}
  
  \CommentTok{\# Create model frame}
\NormalTok{  model\_frame }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(formula, data) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{small\_area =}\NormalTok{ data[[small\_area]])}
  \FunctionTok{colnames}\NormalTok{(model\_frame) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"x"}\NormalTok{, }\StringTok{"small\_area"}\NormalTok{)}
  
  \CommentTok{\# Nest by small area}
\NormalTok{  data\_nested }\OtherTok{\textless{}{-}}\NormalTok{ model\_frame }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =}\NormalTok{ small\_area) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(small\_area) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nest}\NormalTok{()}
  
  \CommentTok{\# Bootstrap}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B)\{}
    \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(model\_frame}\SpecialCharTok{$}\NormalTok{small\_area))) \{}
\NormalTok{      boots[[j]] }\OtherTok{\textless{}{-}} \FunctionTok{sample\_n}\NormalTok{(}
\NormalTok{        data\_nested[[}\DecValTok{2}\NormalTok{]][[j]],}
        \AttributeTok{size =} \FunctionTok{length}\NormalTok{(data\_nested[[}\DecValTok{2}\NormalTok{]][[j]]}\SpecialCharTok{$}\NormalTok{y),}
        \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{      )}
\NormalTok{      boots\_df }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(boots) }
\NormalTok{    \}}
    
\NormalTok{    fit[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{freq\_unit}\NormalTok{(boots\_df, y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\StringTok{"id"}\NormalTok{, pop\_data)}
    
\NormalTok{    mean\_df[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{fitted =}\NormalTok{ fit[[i]]}\SpecialCharTok{$}\NormalTok{eblup}\SpecialCharTok{$}\NormalTok{eblup,}
                               \AttributeTok{subsection =}\NormalTok{ fit[[i]]}\SpecialCharTok{$}\NormalTok{eblup}\SpecialCharTok{$}\NormalTok{domain)}
    \ControlFlowTok{if}\NormalTok{ (i }\SpecialCharTok{\%\%} \DecValTok{50} \SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{}
      \FunctionTok{print}\NormalTok{(i)}
\NormalTok{    \}}
\NormalTok{  \}}
  
  \CommentTok{\# Create final output}
\NormalTok{  final }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(mean\_df) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(subsection) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(fitted, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{  mean\_y }\OtherTok{\textless{}{-}}\NormalTok{ model\_frame }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(small\_area) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_y =} \FunctionTok{mean}\NormalTok{(y, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
  
\NormalTok{  COV }\OtherTok{\textless{}{-}}\NormalTok{ final}\SpecialCharTok{$}\NormalTok{sd }\SpecialCharTok{/}\NormalTok{ mean\_y}\SpecialCharTok{$}\NormalTok{mean\_y}
  \FunctionTok{names}\NormalTok{(COV) }\OtherTok{\textless{}{-}}\NormalTok{ final}\SpecialCharTok{$}\NormalTok{subsection}
  
\NormalTok{  COV}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{fitting-models}{%
\section{Fitting Models}\label{fitting-models}}

\hypertarget{data-set-up-preprocessing}{%
\subsection{Data Set-up \& Preprocessing}\label{data-set-up-preprocessing}}
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(mase)}
\FunctionTok{library}\NormalTok{(hbsae)}
\FunctionTok{library}\NormalTok{(sae)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{intwest }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/subsets/df.csv"}\NormalTok{)}
\NormalTok{tccpop }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/population/tcc\_pop.csv"}\NormalTok{)}
\NormalTok{strata }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/population/strata.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set{-}up strata:}
\NormalTok{intwest }\OtherTok{\textless{}{-}}\NormalTok{ intwest }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{FIAstrat =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    FIAstrat }\SpecialCharTok{==} \StringTok{"1"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Sampled{-}Forest"}\NormalTok{,}
\NormalTok{    FIAstrat }\SpecialCharTok{==} \StringTok{"2"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Sampled{-}Nonforest"}\NormalTok{,}
\NormalTok{    FIAstrat }\SpecialCharTok{==} \StringTok{"0"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Sampled{-}Nonforest"}\NormalTok{,}
\NormalTok{    FIAstrat }\SpecialCharTok{==} \StringTok{"3"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Sampled{-}Nonforest"}\NormalTok{,}
\NormalTok{  )) }
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter out subsections that cause errors in computation}
\NormalTok{no0\_subsections }\OtherTok{\textless{}{-}}\NormalTok{ intwest }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(subsection) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_y =} \FunctionTok{mean}\NormalTok{(BIOLIVE\_TPA),}
            \AttributeTok{mean\_x =} \FunctionTok{mean}\NormalTok{(nlcd11)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(mean\_y }\SpecialCharTok{\textgreater{}} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ mean\_x }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(subsection) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{()}

\NormalTok{intwest\_no0 }\OtherTok{\textless{}{-}}\NormalTok{ intwest }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(subsection }\SpecialCharTok{\%in\%}\NormalTok{ no0\_subsections) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(province }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"M261"}\NormalTok{, }\StringTok{"M334"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(subsection }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"342Fi"}\NormalTok{, }\StringTok{"331Kj"}\NormalTok{, }\StringTok{"342Dh"}\NormalTok{, }\StringTok{"341Dc"}\NormalTok{))) }
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create list of dataframes}
\NormalTok{iw }\OtherTok{\textless{}{-}} \FunctionTok{split}\NormalTok{(intwest\_no0, }\AttributeTok{f =}\NormalTok{ intwest\_no0}\SpecialCharTok{$}\NormalTok{province)}
\end{Highlighting}
\end{Shaded}
\hypertarget{direct-estimation-1}{%
\subsection{Direct Estimation}\label{direct-estimation-1}}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strata }\OtherTok{\textless{}{-}}\NormalTok{ strata }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(zoneid }\SpecialCharTok{\%in\%} \FunctionTok{unique}\NormalTok{(intwest\_no0}\SpecialCharTok{$}\NormalTok{subsection)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{fnf\_no\_water =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      fnf\_no\_water }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{"Sampled{-}Forest"}\NormalTok{,}
\NormalTok{      fnf\_no\_water }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{"Sampled{-}Nonforest"}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{strata }\OtherTok{\textless{}{-}}\NormalTok{ strata }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{zoneprop)}

\CommentTok{\# split strata into list and drop column split by}
\NormalTok{strata\_list }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}
  \FunctionTok{split}\NormalTok{(strata, }\AttributeTok{f =}\NormalTok{ strata}\SpecialCharTok{$}\NormalTok{zoneid),}
  \ControlFlowTok{function}\NormalTok{(strata) \{ strata}\SpecialCharTok{$}\NormalTok{zoneid }\OtherTok{\textless{}{-}} \ConstantTok{NULL}\NormalTok{; strata\}}
\NormalTok{  )}

\CommentTok{\# split subsections into list}
\NormalTok{subsection\_list }\OtherTok{\textless{}{-}} \FunctionTok{split}\NormalTok{(intwest\_no0,}
\NormalTok{                         intwest\_no0}\SpecialCharTok{$}\NormalTok{subsection)}


\NormalTok{post\_strat\_bio }\OtherTok{\textless{}{-}} \FunctionTok{map2}\NormalTok{(}\AttributeTok{.x =}\NormalTok{ subsection\_list,}
                       \AttributeTok{.y =}\NormalTok{ strata\_list,}
                       \AttributeTok{.f =}\NormalTok{ postStrat2\_bio)}
\NormalTok{post\_strat\_ba }\OtherTok{\textless{}{-}} \FunctionTok{map2}\NormalTok{(}\AttributeTok{.x =}\NormalTok{ subsection\_list,}
                      \AttributeTok{.y =}\NormalTok{ strata\_list,}
                      \AttributeTok{.f =}\NormalTok{ postStrat2\_ba)}
\NormalTok{post\_strat\_voln }\OtherTok{\textless{}{-}} \FunctionTok{map2}\NormalTok{(}\AttributeTok{.x =}\NormalTok{ subsection\_list,}
                        \AttributeTok{.y =}\NormalTok{ strata\_list,}
                        \AttributeTok{.f =}\NormalTok{ postStrat2\_voln)}
\NormalTok{post\_strat\_cnt }\OtherTok{\textless{}{-}} \FunctionTok{map2}\NormalTok{(}\AttributeTok{.x =}\NormalTok{ subsection\_list,}
                       \AttributeTok{.y =}\NormalTok{ strata\_list,}
                       \AttributeTok{.f =}\NormalTok{ postStrat2\_cnt)}

\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{ps\_est =} \FunctionTok{c}\NormalTok{(}\FunctionTok{bind\_rows}\NormalTok{(post\_strat\_bio)}\SpecialCharTok{$}\NormalTok{ps\_est,}
             \FunctionTok{bind\_rows}\NormalTok{(post\_strat\_ba)}\SpecialCharTok{$}\NormalTok{ps\_est,}
             \FunctionTok{bind\_rows}\NormalTok{(post\_strat\_cnt)}\SpecialCharTok{$}\NormalTok{ps\_est,}
             \FunctionTok{bind\_rows}\NormalTok{(post\_strat\_voln)}\SpecialCharTok{$}\NormalTok{ps\_est),}
  \AttributeTok{ps\_sd =} \FunctionTok{c}\NormalTok{(}\FunctionTok{bind\_rows}\NormalTok{(post\_strat\_bio)}\SpecialCharTok{$}\NormalTok{sd,}
             \FunctionTok{bind\_rows}\NormalTok{(post\_strat\_ba)}\SpecialCharTok{$}\NormalTok{sd,}
             \FunctionTok{bind\_rows}\NormalTok{(post\_strat\_cnt)}\SpecialCharTok{$}\NormalTok{sd,}
             \FunctionTok{bind\_rows}\NormalTok{(post\_strat\_voln)}\SpecialCharTok{$}\NormalTok{sd),}
  \AttributeTok{subsection =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{names}\NormalTok{(post\_strat\_bio), }\DecValTok{4}\NormalTok{),}
  \AttributeTok{response =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BIOLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_bio)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BALIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_ba)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"CNTLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_cnt)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"VOLNLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_voln)))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dirmean }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{dirmean[}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(iw)] }\OtherTok{\textless{}{-}} 
  \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{         direct\_estimate,}
         \AttributeTok{response =} \StringTok{"BIOLIVE\_TPA"}\NormalTok{, }
         \StringTok{"subsection"}\NormalTok{)}
\NormalTok{dirmean[(}\FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}} 
  \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{        direct\_estimate,}
        \AttributeTok{response =} \StringTok{"BALIVE\_TPA"}\NormalTok{,}
        \StringTok{"subsection"}\NormalTok{)}
\NormalTok{dirmean[(}\DecValTok{2}\SpecialCharTok{*}\FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{3}\SpecialCharTok{*}\FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}} 
  \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{         direct\_estimate,}
         \AttributeTok{response =} \StringTok{"CNTLIVE\_TPA"}\NormalTok{,}
         \StringTok{"subsection"}\NormalTok{)}
\NormalTok{dirmean[(}\DecValTok{3}\SpecialCharTok{*}\FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{4}\SpecialCharTok{*}\FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}} 
  \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{         direct\_estimate,}
         \AttributeTok{response =} \StringTok{"VOLNLIVE\_TPA"}\NormalTok{,}
         \StringTok{"subsection"}\NormalTok{)}

\NormalTok{dir }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(dirmean) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Domain, Direct, CV) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cov\_dirmean =}\NormalTok{ CV }\SpecialCharTok{/} \DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{est\_dirmean =}\NormalTok{ Direct) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{CV) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{response =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BIOLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_bio)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BALIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_ba)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"CNTLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_cnt)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"VOLNLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_voln))))}

\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(dir, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"subsection"} \OtherTok{=} \StringTok{"Domain"}\NormalTok{,}
                        \StringTok{"response"} \OtherTok{=} \StringTok{"response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create CoV for ps{-}estimator}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{cov\_dirps =}\NormalTok{ ps\_sd }\SpecialCharTok{/}\NormalTok{ est\_dirmean}
\NormalTok{  )}

\CommentTok{\# change names and rearrange }
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{est\_dirps =}\NormalTok{ ps\_est,}
         \AttributeTok{sd\_dirps =}\NormalTok{ ps\_sd) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{relocate}\NormalTok{(}\StringTok{"subsection"}\NormalTok{, }\StringTok{"response"}\NormalTok{,}
           \StringTok{"est\_dirps"}\NormalTok{, }\StringTok{"cov\_dirps"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\hypertarget{model-based-estimation}{%
\subsection{Model-Based Estimation}\label{model-based-estimation}}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set up list for area level models}
\NormalTok{ps\_dat }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(est\_dirps, sd\_dirps,}
\NormalTok{                subsection, response) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{var =}\NormalTok{ sd\_dirps}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(}\AttributeTok{est =}\NormalTok{ est\_dirps) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(est, var, subsection, response) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{section =} \FunctionTok{str\_remove\_all}\NormalTok{(subsection, }\StringTok{"[:lower:]"}\NormalTok{),}
    \AttributeTok{province =} \FunctionTok{str\_sub}\NormalTok{(section, }\AttributeTok{end =} \SpecialCharTok{{-}}\DecValTok{2}\NormalTok{))}

\NormalTok{ps\_l }\OtherTok{\textless{}{-}} \FunctionTok{split}\NormalTok{(ps\_dat, }\FunctionTok{list}\NormalTok{(ps\_dat}\SpecialCharTok{$}\NormalTok{province))}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# HB Unit}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{bayes\_unit }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{bayes\_unit[}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(iw)] }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}
\NormalTok{  iw,}
\NormalTok{  hb\_unit,}
  \AttributeTok{formula =}\NormalTok{ BIOLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
  \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
  \AttributeTok{pop\_data =}\NormalTok{ tccpop}
\NormalTok{)}
\NormalTok{bayes\_unit[(}\FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    hb\_unit,}
    \AttributeTok{formula =}\NormalTok{ BALIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
    \AttributeTok{pop\_data =}\NormalTok{ tccpop}
\NormalTok{  )}
\NormalTok{bayes\_unit[(}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    hb\_unit,}
    \AttributeTok{formula =}\NormalTok{ CNTLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
    \AttributeTok{pop\_data =}\NormalTok{ tccpop}
\NormalTok{  )}
\NormalTok{bayes\_unit[(}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    hb\_unit,}
    \AttributeTok{formula =}\NormalTok{ VOLNLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
    \AttributeTok{pop\_data =}\NormalTok{ tccpop}
\NormalTok{  )}

\NormalTok{res\_hbu }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{subsection =} \FunctionTok{names}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(bayes\_unit, EST))),}
\AttributeTok{response =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BIOLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_bio)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BALIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_ba)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"CNTLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_cnt)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"VOLNLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_voln))),}
  \AttributeTok{est\_hb\_unit =} \FunctionTok{unlist}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(bayes\_unit, EST)),}
  \AttributeTok{cov\_hb\_unit =} \FunctionTok{unlist}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(bayes\_unit, hb\_CoV))}
\NormalTok{)}

\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(res\_hbu, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"subsection"} \OtherTok{=} \StringTok{"subsection"}\NormalTok{,}
                        \StringTok{"response"} \OtherTok{=} \StringTok{"response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# HB Area}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{bayes\_area }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{bayes\_area[}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(iw)] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    hb\_area,}
    \AttributeTok{formula =}\NormalTok{ BIOLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
    \AttributeTok{pop\_data =}\NormalTok{ tccpop,}
    \AttributeTok{post\_strat\_data =}\NormalTok{ ps\_dat}
\NormalTok{  )}
\NormalTok{bayes\_area[(}\FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    hb\_area,}
    \AttributeTok{formula =}\NormalTok{ BALIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
    \AttributeTok{pop\_data =}\NormalTok{ tccpop,}
    \AttributeTok{post\_strat\_data =}\NormalTok{ ps\_dat}
\NormalTok{  )}
\NormalTok{bayes\_area[(}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    hb\_area,}
    \AttributeTok{formula =}\NormalTok{ CNTLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
    \AttributeTok{pop\_data =}\NormalTok{ tccpop,}
    \AttributeTok{post\_strat\_data =}\NormalTok{ ps\_dat}
\NormalTok{  )}
\NormalTok{bayes\_area[(}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    hb\_area,}
    \AttributeTok{formula =}\NormalTok{ VOLNLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
    \AttributeTok{pop\_data =}\NormalTok{ tccpop,}
    \AttributeTok{post\_strat\_data =}\NormalTok{ ps\_dat}
\NormalTok{  )}

\NormalTok{aranged\_df }\OtherTok{\textless{}{-}}\NormalTok{ intwest\_no0 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(province }\SpecialCharTok{\%in\%} \FunctionTok{names}\NormalTok{(iw)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(province) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(subsection)}
\NormalTok{hb\_area\_res }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{subsection =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{unique}\NormalTok{(aranged\_df}\SpecialCharTok{$}\NormalTok{subsection), }\DecValTok{4}\NormalTok{),}
  \AttributeTok{est\_hb\_area =} \FunctionTok{unlist}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(bayes\_area, EST)),}
  \AttributeTok{cov\_hb\_area =} \FunctionTok{unlist}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(bayes\_area, hb\_CoV)),}
  \AttributeTok{response =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BIOLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_bio)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BALIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_ba)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"CNTLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_cnt)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"VOLNLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_voln))}
\NormalTok{  )}
\NormalTok{)}

\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(hb\_area\_res, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"subsection"} \OtherTok{=} \StringTok{"subsection"}\NormalTok{,}
                                \StringTok{"response"} \OtherTok{=} \StringTok{"response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Freq Unit}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{frequnit }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{frequnit[}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(iw)] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{         freq\_unit,}
         \AttributeTok{formula =}\NormalTok{ BIOLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
         \StringTok{"subsection"}\NormalTok{,}
\NormalTok{         tccpop)}
\NormalTok{frequnit[(}\FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{         freq\_unit,}
         \AttributeTok{formula =}\NormalTok{ BALIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
         \StringTok{"subsection"}\NormalTok{,}
\NormalTok{         tccpop)}
\NormalTok{frequnit[(}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(iw, }
\NormalTok{         freq\_unit,}
         \AttributeTok{formula =}\NormalTok{ CNTLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
         \StringTok{"subsection"}\NormalTok{, }
\NormalTok{         tccpop)}
\NormalTok{frequnit[(}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{         freq\_unit,}
         \AttributeTok{formula =}\NormalTok{ VOLNLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
         \StringTok{"subsection"}\NormalTok{,}
\NormalTok{         tccpop)}

\NormalTok{frequnit\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))) \{}
\NormalTok{    frequnit\_list[[i]] }\OtherTok{\textless{}{-}}\NormalTok{ frequnit[[i]]}\SpecialCharTok{$}\NormalTok{eblup}
\NormalTok{  \}}

\NormalTok{frequnit\_df }\OtherTok{\textless{}{-}}\NormalTok{ frequnit\_list }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{response =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BIOLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_bio)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BALIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_ba)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"CNTLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_cnt)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"VOLNLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_voln)))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}
    \AttributeTok{subsection =}\NormalTok{ domain,}
    \AttributeTok{est\_freq\_unit =}\NormalTok{ eblup}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{sampsize)}

\CommentTok{\# Coef of variation}
\NormalTok{frequnitcov }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}
\NormalTok{frequnitcov }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{Reduce}\NormalTok{(c,}
       \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{       freq\_unit\_CoV,}
       \AttributeTok{formula =}\NormalTok{ BIOLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
       \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
       \AttributeTok{pop\_data =}\NormalTok{ tccpop,}
       \AttributeTok{B =} \DecValTok{500}\NormalTok{)),}
  \FunctionTok{Reduce}\NormalTok{(c,}
       \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{       freq\_unit\_CoV,}
       \AttributeTok{formula =}\NormalTok{ BALIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
       \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
       \AttributeTok{pop\_data =}\NormalTok{ tccpop,}
       \AttributeTok{B =} \DecValTok{500}\NormalTok{)),}
  \FunctionTok{Reduce}\NormalTok{(c,}
       \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{       freq\_unit\_CoV,}
       \AttributeTok{formula =}\NormalTok{ CNTLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
       \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
       \AttributeTok{pop\_data =}\NormalTok{ tccpop,}
       \AttributeTok{B =} \DecValTok{500}\NormalTok{)),}
  \FunctionTok{Reduce}\NormalTok{(c,}
       \FunctionTok{lapply}\NormalTok{(iw,}
\NormalTok{       freq\_unit\_CoV,}
       \AttributeTok{formula =}\NormalTok{ VOLNLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
       \AttributeTok{small\_area =} \StringTok{"subsection"}\NormalTok{,}
       \AttributeTok{pop\_data =}\NormalTok{ tccpop,}
       \AttributeTok{B =} \DecValTok{500}\NormalTok{))}
\NormalTok{)}

\NormalTok{frequnitcov\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{cov\_freq\_unit =}\NormalTok{ frequnitcov,}
  \AttributeTok{subsection =} \FunctionTok{names}\NormalTok{(frequnitcov),}
  \AttributeTok{response =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BIOLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_bio)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BALIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_ba)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"CNTLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_cnt)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"VOLNLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_voln))}
\NormalTok{  )}
\NormalTok{)}

\NormalTok{frequnit\_df }\OtherTok{\textless{}{-}}\NormalTok{ frequnit\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(frequnitcov\_df,}
            \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"subsection"} \OtherTok{=} \StringTok{"subsection"}\NormalTok{,}
                   \StringTok{"response"} \OtherTok{=} \StringTok{"response"}\NormalTok{))}

\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(frequnit\_df,}
            \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"subsection"} \OtherTok{=} \StringTok{"subsection"}\NormalTok{,}
                   \StringTok{"response"} \OtherTok{=} \StringTok{"response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Freq Area}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{freqarea }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{freqarea[}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(iw)] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    freq\_area,}
    \AttributeTok{formula =}\NormalTok{ BIOLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \StringTok{"subsection"}\NormalTok{,}
\NormalTok{    tccpop,}
    \AttributeTok{post\_strat\_data =}\NormalTok{ ps\_dat}
\NormalTok{)}
\NormalTok{freqarea[(}\FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}} 
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    freq\_area,}
    \AttributeTok{formula =}\NormalTok{ BALIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \StringTok{"subsection"}\NormalTok{,}
\NormalTok{    tccpop,}
    \AttributeTok{post\_strat\_data =}\NormalTok{ ps\_dat}
\NormalTok{)}
\NormalTok{freqarea[(}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    freq\_area,}
    \AttributeTok{formula =}\NormalTok{ CNTLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \StringTok{"subsection"}\NormalTok{,}
\NormalTok{    tccpop,}
    \AttributeTok{post\_strat\_data =}\NormalTok{ ps\_dat}
\NormalTok{)}
\NormalTok{freqarea[(}\DecValTok{3} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))] }\OtherTok{\textless{}{-}}
  \FunctionTok{lapply}\NormalTok{(}
\NormalTok{    iw,}
\NormalTok{    freq\_area,}
    \AttributeTok{formula =}\NormalTok{ VOLNLIVE\_TPA }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nlcd11,}
    \StringTok{"subsection"}\NormalTok{,}
\NormalTok{    tccpop,}
    \AttributeTok{post\_strat\_data =}\NormalTok{ ps\_dat}
\NormalTok{)}

\NormalTok{freqarea\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))) \{}
\NormalTok{  freqarea\_list[[i]] }\OtherTok{\textless{}{-}}\NormalTok{ freqarea[[i]]}\SpecialCharTok{$}\NormalTok{est}\SpecialCharTok{$}\NormalTok{eblup}
\NormalTok{\}}
\NormalTok{freqarea\_cov\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(}\DecValTok{4} \SpecialCharTok{*} \FunctionTok{length}\NormalTok{(iw))) \{}
\NormalTok{  freqarea\_cov\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(freqarea[[i]]}\SpecialCharTok{$}\NormalTok{mse)}
\NormalTok{\}}

\NormalTok{freq\_area\_res }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{subsection =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{unique}\NormalTok{(aranged\_df}\SpecialCharTok{$}\NormalTok{subsection), }\DecValTok{4}\NormalTok{),}
  \AttributeTok{est\_freq\_area =} \FunctionTok{unlist}\NormalTok{(freqarea\_list),}
  \AttributeTok{se\_freq\_area =} \FunctionTok{unlist}\NormalTok{(freqarea\_cov\_list),}
  \AttributeTok{response =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BIOLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_bio)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"BALIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_ba)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"CNTLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_cnt)),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"VOLNLIVE\_TPA"}\NormalTok{, }\FunctionTok{length}\NormalTok{(post\_strat\_voln))}
\NormalTok{  )}
\NormalTok{)}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(freq\_area\_res,}
            \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"subsection"} \OtherTok{=} \StringTok{"subsection"}\NormalTok{,}
                   \StringTok{"response"} \OtherTok{=} \StringTok{"response"}\NormalTok{))}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cov\_freq\_area =}\NormalTok{ se\_freq\_area }\SpecialCharTok{/}\NormalTok{ est\_dirmean)}
\end{Highlighting}
\end{Shaded}
\hypertarget{writing-data-files-pivoting-to-tidy-format}{%
\subsection{Writing Data Files \& Pivoting to Tidy Format}\label{writing-data-files-pivoting-to-tidy-format}}
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(results, }
          \StringTok{"data/results/final\_results.csv"}\NormalTok{)}
\FunctionTok{saveRDS}\NormalTok{(results,}
        \AttributeTok{file =} \StringTok{"data/results/final\_results.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimates\_long }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"est\_hb\_unit"}\NormalTok{, }\StringTok{"est\_hb\_area"}\NormalTok{,}
                        \StringTok{"est\_freq\_unit"}\NormalTok{, }\StringTok{"est\_freq\_area"}\NormalTok{, }
                        \StringTok{"est\_dirmean"}\NormalTok{, }\StringTok{"est\_dirps"}\NormalTok{),}
               \AttributeTok{names\_to =} \StringTok{"estimator"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"estimate"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{cov\_hb\_unit, }\SpecialCharTok{{-}}\NormalTok{cov\_hb\_area,}
                \SpecialCharTok{{-}}\NormalTok{cov\_freq\_unit, }\SpecialCharTok{{-}}\NormalTok{cov\_freq\_area,}
                \SpecialCharTok{{-}}\NormalTok{cov\_dirmean, }\SpecialCharTok{{-}}\NormalTok{cov\_dirps) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{estimator =}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_sub}\NormalTok{(estimator, }\AttributeTok{start =} \DecValTok{5}\NormalTok{))}

\NormalTok{cov\_long }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"cov\_hb\_unit"}\NormalTok{, }\StringTok{"cov\_hb\_area"}\NormalTok{,}
                        \StringTok{"cov\_freq\_unit"}\NormalTok{, }\StringTok{"cov\_freq\_area"}\NormalTok{,}
                        \StringTok{"cov\_dirmean"}\NormalTok{, }\StringTok{"cov\_dirps"}\NormalTok{),}
               \AttributeTok{names\_to =} \StringTok{"estimator"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"cov"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{est\_hb\_unit, }\SpecialCharTok{{-}}\NormalTok{est\_hb\_area,}
                \SpecialCharTok{{-}}\NormalTok{est\_freq\_unit, }\SpecialCharTok{{-}}\NormalTok{est\_freq\_area,}
                \SpecialCharTok{{-}}\NormalTok{est\_dirmean, }\SpecialCharTok{{-}}\NormalTok{est\_dirps) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{estimator =}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_sub}\NormalTok{(estimator, }\AttributeTok{start =} \DecValTok{5}\NormalTok{))}

\NormalTok{final\_results\_long }\OtherTok{\textless{}{-}}\NormalTok{ estimates\_long }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{full\_join}\NormalTok{(cov\_long) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{section =} \FunctionTok{str\_remove\_all}\NormalTok{(subsection, }\StringTok{"[:lower:]"}\NormalTok{),}
    \AttributeTok{province =} \FunctionTok{str\_sub}\NormalTok{(section, }\AttributeTok{end =} \SpecialCharTok{{-}}\DecValTok{2}\NormalTok{)}
\NormalTok{  )}

\NormalTok{final\_results\_long }\OtherTok{\textless{}{-}}\NormalTok{ final\_results\_long }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{sd\_dirps, }\SpecialCharTok{{-}}\NormalTok{se\_freq\_area)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(final\_results\_long,}
          \StringTok{"data/results/final\_results\_long.csv"}\NormalTok{)}
\FunctionTok{saveRDS}\NormalTok{(final\_results\_long,}
        \AttributeTok{file =} \StringTok{"data/results/final\_results\_long.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\backmatter

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\noindent

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-bayes-orig}{}}%
Bayes, T. (1763). LII. An essay towards solving a problem in the doctrine of chances. By the late rev. Mr. Bayes, FRS communicated by mr. Price, in a letter to john canton, AMFR s. \emph{Philosophical Transactions of the Royal Society of London}, (53), 370--418.

\leavevmode\vadjust pre{\hypertarget{ref-hbsae-package}{}}%
Boonstra, H. J. (2012). \emph{Hbsae: Hierarchical bayesian small area estimation}. Retrieved from \url{https://CRAN.R-project.org/package=hbsae}

\leavevmode\vadjust pre{\hypertarget{ref-andrew-392}{}}%
Bray, A. (2020). Math 392: Mathematical statistics.

\leavevmode\vadjust pre{\hypertarget{ref-breidenbach2012}{}}%
Breidenbach, J., \& Astrup, R. (2012). Small area estimation of forest attributes in the norwegian national forest inventory. \emph{European Journal of Forest Research}, \emph{131}(4), 1255--1267.

\leavevmode\vadjust pre{\hypertarget{ref-bootstrap}{}}%
Efron, B. (1992). Bootstrap methods: Another look at the jackknife. In \emph{Breakthroughs in statistics} (pp. 569--593). Springer.

\leavevmode\vadjust pre{\hypertarget{ref-whatisfia}{}}%
FIA. (2020). Forest inventory and analysis national program. \emph{What is FIA?} Retrieved from \url{https://www.fia.fs.fed.us/about/about_us/}

\leavevmode\vadjust pre{\hypertarget{ref-mse-area}{}}%
Hidiroglou, M., \& You, Y. (2016). Comparison of unit level and area level small area estimators, \emph{42}, 41--61.

\leavevmode\vadjust pre{\hypertarget{ref-nlcd11}{}}%
Homer, C. (2015, November). Completion of the 2011 national land cover database for the conterminous united states -- representing a decade of land cover change information. \emph{EPA}. Environmental Protection Agency. Retrieved from \url{https://cfpub.epa.gov/si/si_public_record_report.cfm?Lab=NERL\&dirEntryId=309950}

\leavevmode\vadjust pre{\hypertarget{ref-doves}{}}%
Hooten, M. B., \& Wikle, C. K. (2008). A hierarchical bayesian non-linear spatio-temporal model for the spread of invasive species with application to the eurasian collared-dove. \emph{Environmental and Ecological Statistics}, \emph{15}(1), 59--70.

\leavevmode\vadjust pre{\hypertarget{ref-hor52}{}}%
Horvitz, D. G., \& Thompson, D. J. (1952). A generalization of sampling without replacement from a finite universe. \emph{Journal of the American Statistical Association}, \emph{47}, 663--685.

\leavevmode\vadjust pre{\hypertarget{ref-inbook}{}}%
Iriawan, N., \& Yasmirullah, S. (2019). An economic growth model using hierarchical bayesian method. http://doi.org/\href{https://doi.org/10.5772/intechopen.88650}{10.5772/intechopen.88650}

\leavevmode\vadjust pre{\hypertarget{ref-mcconville2020}{}}%
McConville, K. S., Moisen, G. G., \& Frescino, T. S. (2020). A tutorial on model-assisted estimation with application to forest inventory. \emph{Forests}, \emph{11}(2).

\leavevmode\vadjust pre{\hypertarget{ref-mase}{}}%
McConville, K., Tang, B., Zhu, G., Cheung, S., \& Li, S. (2018). \emph{Mase: Model-assisted survey estimation}. Retrieved from \url{https://cran.r-project.org/package=mase}

\leavevmode\vadjust pre{\hypertarget{ref-statrethinking}{}}%
McElreath, R. (2020). \emph{Statistical rethinking: A bayesian course with examples in r and STAN}.

\leavevmode\vadjust pre{\hypertarget{ref-sae-package}{}}%
Molina, I., \& Marhuenda, Y. (2015). {sae}: An {R} package for small area estimation. \emph{The R Journal}, \emph{7}(1), 81--98. Retrieved from \url{https://journal.r-project.org/archive/2015/RJ-2015-007/RJ-2015-007.pdf}

\leavevmode\vadjust pre{\hypertarget{ref-r-software}{}}%
R Core Team. (2020). \emph{R: A language and environment for statistical computing}. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from \url{https://www.R-project.org/}

\leavevmode\vadjust pre{\hypertarget{ref-rao2014}{}}%
Rao, J. N. (2014). Small-area estimation. \emph{Wiley StatsRef: Statistics Reference Online}.

\leavevmode\vadjust pre{\hypertarget{ref-med}{}}%
Tojtovska, B., Ribarski, P., \& Ljubic, A. (2019). Application of hierarchical bayesian model in ophtalmological study. In S. Gievska \& G. Madjarov (Eds.), \emph{ICT innovations 2019. Big data processing and mining} (pp. 109--120). Cham: Springer International Publishing.

\leavevmode\vadjust pre{\hypertarget{ref-ver2017}{}}%
Ver Planck, N. R., Finley, A. O., \& Huff, E. S. (2017). Hierarchical bayesian models for small area estimation of county-level private forest landowner population. \emph{Canadian Journal of Forest Research}, \emph{47}(12), 1577--1589.

\leavevmode\vadjust pre{\hypertarget{ref-tidyverse}{}}%
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., Franois, R., \ldots{} Yutani, H. (2019). Welcome to the {tidyverse}. \emph{Journal of Open Source Software}, \emph{4}(43), 1686. http://doi.org/\href{https://doi.org/10.21105/joss.01686}{10.21105/joss.01686}

\end{CSLReferences}

% Index?

\end{document}
